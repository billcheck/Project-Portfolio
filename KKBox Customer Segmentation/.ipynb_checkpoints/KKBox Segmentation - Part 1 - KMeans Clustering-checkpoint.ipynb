{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKBox Customer Churn Prediction\n",
    "### w/ BigQuery and Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: <font color=green>*Model Creation and Evaluation*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "from __future__ import absolute_import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Evaluation Imports\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports for PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "# import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# # Imports for BigQuery connection\n",
    "# import json\n",
    "# import pprint\n",
    "# import subprocess\n",
    "\n",
    "# # Imports for GCP\n",
    "# from google.cloud import bigquery\n",
    "import time \n",
    "# import gcsfs\n",
    "\n",
    "# Imports for Spark ML\n",
    "from pyspark.ml.feature import (VectorAssembler, OneHotEncoderEstimator, OneHotEncoder)\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataproc Specs\n",
    "\n",
    "# Jupyter Initialization: gs://srcd-dataproc/jupyter.sh \n",
    "# Components Installed: Anaconda and Jupyter\n",
    "# Master Node:   x1 - 4 vCPU w/ 15 GB RAM each\n",
    "# Workers Nodes: x5 - 4 vCPU w/ 15 GB RAM each\n",
    "# Disk: 100GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session and Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Google Credentials\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='D:\\OneDrive\\J-5\\GitHub\\Google Credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.Builder().config(conf=SparkConf().setMaster(\"local[*]\")).getOrCreate()\n",
    "\n",
    "# Instantiate BigQuery magic\n",
    "# %load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[msno: string, membership_expire_date: timestamp, payment_method_id: int, payment_plan_days: int, plan_list_price: int, net_paid_amount: int, is_net_paid_amount: string, is_auto_renew: int, is_cancel: int, city: int, bd: int, registered_via: int, registration_init_time: timestamp, membership_length: int, is_churn: int, total_songs: int, total_logins: int, total_secs: double, sum_num_unq: int, sum_num_repeat: int, sum_over_50pec: int, sum_over_75pec: int, sum_over_985pec: int, total_transactions: int, total_spent: int, avg_spent_trans: double, spent_per_logins: double, spent_per_secs: double, spent_per_song: double, spent_per_num_unq: double, spent_per_num_repeats: double, never_active_subscriber: int, total_spent_zero: int, city_agg: int, payment_method_agg: int, expire_last_login: int, total_cancelations: int, songs_last_7: int, songs_last_7_AVG: double, logins_last_7: int, logins_last_7_AVG: double, total_secs_last_7: double, total_secs_last_7_AVG: double, num_unq_last_7: int, num_unq_last_7_AVG: double, num_repeat_last_7: int, num_repeat_last_7_AVG: double, over_50perc_last_7: int, over_50perc_last_7_AVG: double, over_75perc_last_7: int, over_75perc_last_7_AVG: double, over_985perc_last_7: int, over_985perc_last_7_AVG: double, songs_last_15: int, songs_last_15_AVG: double, logins_last_15: int, logins_last_15_AVG: double, total_secs_last_15: double, total_secs_last_15_AVG: double, num_unq_last_15: int, num_unq_last_15_AVG: double, num_repeat_last_15: int, num_repeat_last_15_AVG: double, over_50perc_last_15: int, over_50perc_last_15_AVG: double, over_75perc_last_15: int, over_75perc_last_15_AVG: double, over_985perc_last_15: int, over_985perc_last_15_AVG: double, songs_last_30: int, songs_last_30_AVG: double, logins_last_30: int, logins_last_30_AVG: double, total_secs_last_30: double, total_secs_last_30_AVG: double, num_unq_last_30: int, num_unq_last_30_AVG: double, num_repeat_last_30: int, num_repeat_last_30_AVG: double, over_50perc_last_30: int, over_50perc_last_30_AVG: double, over_75perc_last_30: int, over_75perc_last_30_AVG: double, over_985perc_last_30: int, over_985perc_last_30_AVG: double, songs_last_60: int, songs_last_60_AVG: double, logins_last_60: int, logins_last_60_AVG: double, total_secs_last_60: double, total_secs_last_60_AVG: double, num_unq_last_60: int, num_unq_last_60_AVG: double, num_repeat_last_60: int, num_repeat_last_60_AVG: double, over_50perc_last_60: int, over_50perc_last_60_AVG: double, over_75perc_last_60: int, over_75perc_last_60_AVG: double, over_985perc_last_60: int, over_985perc_last_60_AVG: double, songs_last_120: int, songs_last_120_AVG: double, logins_last_120: int, logins_last_120_AVG: double, total_secs_last_120: double, total_secs_last_120_AVG: double, num_unq_last_120: int, num_unq_last_120_AVG: double, num_repeat_last_120: int, num_repeat_last_120_AVG: double, over_50perc_last_120: int, over_50perc_last_120_AVG: double, over_75perc_last_120: int, over_75perc_last_120_AVG: double, over_985perc_last_120: int, over_985perc_last_120_AVG: double, login_after_expire_10: int, login_after_expire_20: int, login_after_expire_30: int, SUM_unq_songs_0_15: int, AVG_unq_songs_0_15: double, STD_unq_songs_0_15: double, SUM_songs_0_15: int, AVG_songs_0_15: double, STD_songs_0_15: double, SUM_secs_0_15: double, AVG_secs_0_15: double, STD_secs_0_15: double, SUM_songs50_0_15: int, AVG_songs50_0_15: double, STD_songs50_0_15: double, SUM_logins_0_15: int, AVG_logins_0_15: double, SUM_repeats_0_15: int, AVG_repeats_0_15: double, STD_repeats_0_15: double, SUM_unq_songs_15_30: int, AVG_unq_songs_15_30: double, STD_unq_songs_15_30: double, SUM_songs_15_30: int, AVG_songs_15_30: double, STD_songs_15_30: double, SUM_secs_15_30: double, AVG_secs_15_30: double, STD_secs_15_30: double, SUM_songs50_15_30: int, AVG_songs50_15_30: double, STD_songs50_15_30: double, SUM_logins_15_30: int, AVG_logins_15_30: double, SUM_repeats_15_30: int, AVG_repeats_15_30: double, STD_repeats_15_30: double, SUM_unq_songs_30_45: int, AVG_unq_songs_30_45: double, STD_unq_songs_30_45: double, SUM_songs_30_45: int, AVG_songs_30_45: double, STD_songs_30_45: double, SUM_secs_30_45: double, AVG_secs_30_45: double, STD_secs_30_45: double, SUM_songs50_30_45: int, AVG_songs50_30_45: double, STD_songs50_30_45: double, SUM_logins_30_45: int, AVG_logins_30_45: double, SUM_repeats_30_45: int, AVG_repeats_30_45: double, STD_repeats_30_45: double, SUM_unq_songs_45_60: int, AVG_unq_songs_45_60: double, STD_unq_songs_45_60: double, SUM_songs_45_60: int, AVG_songs_45_60: double, STD_songs_45_60: double, SUM_secs_45_60: double, AVG_secs_45_60: double, STD_secs_45_60: double, SUM_songs50_45_60: int, AVG_songs50_45_60: double, STD_songs50_45_60: double, SUM_logins_45_60: int, AVG_logins_45_60: double, SUM_repeats_45_60: int, AVG_repeats_45_60: double, STD_repeats_45_60: double, DIFSUM_unq_songs_0_15_15_30: int, DIFAVG_unq_songs_0_15_15_30: double, DIFSTD_unq_songs_0_15_15_30: double, DIFSUM_songs_0_15_15_30: int, DIFAVG_songs_0_15_15_30: double, DIFSTD_songs_0_15_15_30: double, DIFSUM_secs_0_15_15_30: double, DIFAVG_secs_0_15_15_30: double, DIFSTD_secs_0_15_15_30: double, DIFSUM_songs50_0_15_15_30: int, DIFAVG_songs50_0_15_15_30: double, DIFSTD_songs50_0_15_15_30: double, DIFSUM_logins_0_15_15_30: int, DIFAVG_logins_0_15_15_30: double, DIFSUM_repeats_0_15_15_30: int, DIFAVG_repeats_0_15_15_30: double, DIFSTD_repeats_0_15_15_30: double, DIFSUM_unq_songs_15_30_30_45: int, DIFAVG_unq_songs_15_30_30_45: double, DIFSTD_unq_songs_15_30_30_45: double, DIFSUM_songs_15_30_30_45: int, DIFAVG_songs_15_30_30_45: double, DIFSTD_songs_15_30_30_45: double, DIFSUM_secs_15_30_30_45: double, DIFAVG_secs_15_30_30_45: double, DIFSTD_secs_15_30_30_45: double, DIFSUM_songs50_15_30_30_45: int, DIFAVG_songs50_15_30_30_45: double, DIFSTD_songs50_15_30_30_45: double, DIFSUM_logins_15_30_30_45: int, DIFAVG_logins_15_30_30_45: double, DIFSUM_repeats_15_30_30_45: int, DIFAVG_repeats_15_30_30_45: double, DIFSTD_repeats_15_30_30_45: double, DIFSUM_unq_songs_30_45_45_60: int, DIFAVG_unq_songs_30_45_45_60: double, DIFSTD_unq_songs_30_45_45_60: double, DIFSUM_songs_30_45_45_60: int, DIFAVG_songs_30_45_45_60: double, DIFSTD_songs_30_45_45_60: double, DIFSUM_secs_30_45_45_60: double, DIFAVG_secs_30_45_45_60: double, DIFSTD_secs_30_45_45_60: double, DIFSUM_songs50_30_45_45_60: int, DIFAVG_songs50_30_45_45_60: double, DIFSTD_songs50_30_45_45_60: double, DIFSUM_logins_30_45_45_60: int, DIFAVG_logins_30_45_45_60: double, DIFSUM_repeats_30_45_45_60: int, DIFAVG_repeats_30_45_45_60: double, DIFSTD_repeats_30_45_45_60: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # If Working Locally on Computer, Importing Data Locally#\n",
    "\n",
    "# Import DRV_Jan2016 (Train Set) \n",
    "\n",
    "DRV_Jan20160 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016000000000000',inferSchema=True,header=True)\n",
    "DRV_Jan20161 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016000000000001',inferSchema=True,header=True)\n",
    "DRV_Jan20162 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016000000000002',inferSchema=True,header=True)\n",
    "\n",
    "DRV_Jan2016 = DRV_Jan20160.union(DRV_Jan20161)\n",
    "DRV_Jan2016 = DRV_Jan2016.union(DRV_Jan20162)\n",
    "\n",
    "DRV_Jan20160.unpersist()\n",
    "DRV_Jan20161.unpersist()\n",
    "DRV_Jan20162.unpersist()\n",
    "\n",
    "# # Import DRV_Feb2016 (Validation Set) \n",
    "# DRV_Feb20160 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Feb2016000000000000',inferSchema=True,header=True)\n",
    "# DRV_Feb20161 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Feb2016000000000001',inferSchema=True,header=True)\n",
    "# DRV_Feb20162 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Feb2016000000000002',inferSchema=True,header=True)\n",
    "\n",
    "# DRV_Feb2016 = DRV_Feb20160.union(DRV_Feb20161)\n",
    "# DRV_Feb2016 = DRV_Feb2016.union(DRV_Feb20162)\n",
    "\n",
    "# DRV_Feb20160.unpersist()\n",
    "# DRV_Feb20161.unpersist()\n",
    "# DRV_Feb20162.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DRV_Jan2016 as Pandas \n",
    "DRV_Jan20160 = pd.read_csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016000000000000')\n",
    "DRV_Jan20161 = pd.read_csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016000000000001')\n",
    "DRV_Jan20162 = pd.read_csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016000000000002')\n",
    "\n",
    "DRV_Jan2016x = pd.concat([DRV_Jan20160,DRV_Jan20161,DRV_Jan20162])\n",
    "\n",
    "del DRV_Jan20160\n",
    "del DRV_Jan20161\n",
    "del DRV_Jan20162\n",
    "\n",
    "# Drop columns\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('msno', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('is_churn', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('membership_expire_date', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('registration_init_time', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('city', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('bd', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('payment_method_id', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('is_net_paid_amount', axis=1)\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop('registered_via', axis=1)\n",
    "\n",
    "\n",
    "DRV_Jan2016x = DRV_Jan2016x.drop(['is_auto_renew', 'total_spent_zero', 'city_agg', \n",
    "                   'payment_method_agg', 'never_active_subscriber'],\n",
    "                   axis=1)\n",
    "\n",
    "# Scale the data.\n",
    "X_scaled = StandardScaler().fit_transform(DRV_Jan2016x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ## If Working Locally on Computer, Importing Data from GCS ##\n",
    "\n",
    "# # Import DRV_Jan2016 (Train Set) from Google Cloud Storage via Pandas\n",
    "# DRV_Jan2016_Balanced_1 = spark.createDataFrame(pd.read_csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_Balanced_1'))\n",
    "# DRV_Jan2016_Balanced_2 = spark.createDataFrame(pd.read_csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_Balanced_2'))\n",
    "# DRV_Jan2016_Balanced_3 = spark.createDataFrame(pd.read_csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_Balanced_3'))\n",
    "\n",
    "# # Import DRV_Feb2016 (Validation Set) from Google Cloud Storage via Pandas\n",
    "# DRV_Feb2016 = spark.createDataFrame(pd.read_csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Feb2016'))\n",
    "\n",
    "# # Import DRV_Mar2016 (Test Set) from Google Cloud Storage via Pandas\n",
    "# DRV_Mar2016 = spark.createDataFrame(pd.read_csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Mar2016'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # If Working on Dataproc Cloud ##\n",
    "\n",
    "# # Import DRV_Jan2016 (Train Set) \n",
    "# DRV_Jan2016_1to1 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_1to1',inferSchema=True,header=True)\n",
    "# DRV_Jan2016_3to1 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_3to1',inferSchema=True,header=True)\n",
    "# DRV_Jan2016_5to1 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_5to1',inferSchema=True,header=True)\n",
    "# DRV_Jan2016_7to1 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_7to1',inferSchema=True,header=True)\n",
    "# DRV_Jan2016_9to1 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_9to1',inferSchema=True,header=True)\n",
    "# DRV_Jan2016_11to1 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_11to1',inferSchema=True,header=True)\n",
    "# DRV_Jan2016_13to1 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016_13to1',inferSchema=True,header=True)\n",
    "\n",
    "# DRV_Jan20160 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016000000000000',inferSchema=True,header=True)\n",
    "# DRV_Jan20161 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016000000000001',inferSchema=True,header=True)\n",
    "# DRV_Jan20162 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Jan2016000000000002',inferSchema=True,header=True)\n",
    "\n",
    "# DRV_Jan2016 = DRV_Jan20160.union(DRV_Jan20161)\n",
    "# DRV_Jan2016 = DRV_Jan2016.union(DRV_Jan20162)\n",
    "\n",
    "# DRV_Jan20160 = None\n",
    "# DRV_Jan20161 = None\n",
    "# DRV_Jan20162 = None\n",
    "\n",
    "# # Import DRV_Feb2016 (Validation Set) \n",
    "# DRV_Feb20160 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Feb2016000000000000',inferSchema=True,header=True)\n",
    "# DRV_Feb20161 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Feb2016000000000001',inferSchema=True,header=True)\n",
    "# DRV_Feb20162 = spark.read.csv('gs://dataproc-fb3fa26d-011a-4757-afb9-5efdd6e75d60-us-east1/Datasets/KKBox User Data/Monthly Datasets/DRV_Feb2016000000000002',inferSchema=True,header=True)\n",
    "\n",
    "# DRV_Feb2016 = DRV_Feb20160.union(DRV_Feb20161)\n",
    "# DRV_Feb2016 = DRV_Feb2016.union(DRV_Feb20162)\n",
    "\n",
    "# DRV_Feb20160 = None\n",
    "# DRV_Feb20161 = None\n",
    "# DRV_Feb20162 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cast Correct Column Types on All Sets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "column_types_pd = [('msno', 'STRING'),\n",
    " ('membership_expire_date', 'DATE'),\n",
    " ('payment_method_id', 'INT64'),\n",
    " ('payment_plan_days', 'INT64'),\n",
    " ('plan_list_price', 'INT64'),\n",
    " ('net_paid_amount', 'INT64'),\n",
    " ('is_net_paid_amount', 'STRING'),\n",
    " ('is_auto_renew', 'INT64'),\n",
    " ('city', 'INT64'),\n",
    " ('bd', 'INT64'),\n",
    " ('registered_via', 'INT64'),\n",
    " ('registration_init_time', 'DATE'),\n",
    " ('membership_length', 'INT64'),\n",
    " ('is_churn', 'FLOAT64'),\n",
    " ('total_songs', 'INT64'),\n",
    " ('total_logins', 'INT64'),\n",
    " ('total_secs', 'FLOAT64'),\n",
    " ('sum_num_unq', 'INT64'),\n",
    " ('sum_num_repeat', 'INT64'),\n",
    " ('sum_over_50pec', 'INT64'),\n",
    " ('sum_over_75pec', 'INT64'),\n",
    " ('sum_over_985pec', 'INT64'),\n",
    " ('total_transactions', 'INT64'),\n",
    " ('total_spent', 'FLOAT64'),\n",
    " ('avg_spent_trans', 'FLOAT64'),\n",
    " ('spent_per_logins', 'FLOAT64'),\n",
    " ('spent_per_secs', 'FLOAT64'),\n",
    " ('spent_per_song', 'FLOAT64'),\n",
    " ('spent_per_num_unq', 'FLOAT64'),\n",
    " ('spent_per_num_repeats', 'FLOAT64'),\n",
    " ('never_active_subscriber', 'FLOAT64'),\n",
    " ('total_spent_zero', 'FLOAT64'),\n",
    " ('city_agg', 'INT64'),\n",
    " ('payment_method_agg', 'INT64'),\n",
    " ('songs_last_7', 'FLOAT64'),\n",
    " ('songs_last_7_AVG', 'FLOAT64'),\n",
    " ('logins_last_7', 'FLOAT64'),\n",
    " ('logins_last_7_AVG', 'FLOAT64'),\n",
    " ('total_secs_last_7', 'FLOAT64'),\n",
    " ('total_secs_last_7_AVG', 'FLOAT64'),\n",
    " ('num_unq_last_7', 'FLOAT64'),\n",
    " ('num_unq_last_7_AVG', 'FLOAT64'),\n",
    " ('num_repeat_last_7', 'FLOAT64'),\n",
    " ('num_repeat_last_7_AVG', 'FLOAT64'),\n",
    " ('over_50perc_last_7', 'FLOAT64'),\n",
    " ('over_50perc_last_7_AVG', 'FLOAT64'),\n",
    " ('over_75perc_last_7', 'FLOAT64'),\n",
    " ('over_75perc_last_7_AVG', 'FLOAT64'),\n",
    " ('over_985perc_last_7', 'FLOAT64'),\n",
    " ('over_985perc_last_7_AVG', 'FLOAT64'),\n",
    " ('songs_last_15', 'FLOAT64'),\n",
    " ('songs_last_15_AVG', 'FLOAT64'),\n",
    " ('logins_last_15', 'FLOAT64'),\n",
    " ('logins_last_15_AVG', 'FLOAT64'),\n",
    " ('total_secs_last_15', 'FLOAT64'),\n",
    " ('total_secs_last_15_AVG', 'FLOAT64'),\n",
    " ('num_unq_last_15', 'FLOAT64'),\n",
    " ('num_unq_last_15_AVG', 'FLOAT64'),\n",
    " ('num_repeat_last_15', 'FLOAT64'),\n",
    " ('num_repeat_last_15_AVG', 'FLOAT64'),\n",
    " ('over_50perc_last_15', 'FLOAT64'),\n",
    " ('over_50perc_last_15_AVG', 'FLOAT64'),\n",
    " ('over_75perc_last_15', 'FLOAT64'),\n",
    " ('over_75perc_last_15_AVG', 'FLOAT64'),\n",
    " ('over_985perc_last_15', 'FLOAT64'),\n",
    " ('over_985perc_last_15_AVG', 'FLOAT64'),\n",
    " ('songs_last_30', 'FLOAT64'),\n",
    " ('songs_last_30_AVG', 'FLOAT64'),\n",
    " ('logins_last_30', 'FLOAT64'),\n",
    " ('logins_last_30_AVG', 'FLOAT64'),\n",
    " ('total_secs_last_30', 'FLOAT64'),\n",
    " ('total_secs_last_30_AVG', 'FLOAT64'),\n",
    " ('num_unq_last_30', 'FLOAT64'),\n",
    " ('num_unq_last_30_AVG', 'FLOAT64'),\n",
    " ('num_repeat_last_30', 'FLOAT64'),\n",
    " ('num_repeat_last_30_AVG', 'FLOAT64'),\n",
    " ('over_50perc_last_30', 'FLOAT64'),\n",
    " ('over_50perc_last_30_AVG', 'FLOAT64'),\n",
    " ('over_75perc_last_30', 'FLOAT64'),\n",
    " ('over_75perc_last_30_AVG', 'FLOAT64'),\n",
    " ('over_985perc_last_30', 'FLOAT64'),\n",
    " ('over_985perc_last_30_AVG', 'FLOAT64'),\n",
    " ('songs_last_60', 'FLOAT64'),\n",
    " ('songs_last_60_AVG', 'FLOAT64'),\n",
    " ('logins_last_60', 'FLOAT64'),\n",
    " ('logins_last_60_AVG', 'FLOAT64'),\n",
    " ('total_secs_last_60', 'FLOAT64'),\n",
    " ('total_secs_last_60_AVG', 'FLOAT64'),\n",
    " ('num_unq_last_60', 'FLOAT64'),\n",
    " ('num_unq_last_60_AVG', 'FLOAT64'),\n",
    " ('num_repeat_last_60', 'FLOAT64'),\n",
    " ('num_repeat_last_60_AVG', 'FLOAT64'),\n",
    " ('over_50perc_last_60', 'FLOAT64'),\n",
    " ('over_50perc_last_60_AVG', 'FLOAT64'),\n",
    " ('over_75perc_last_60', 'FLOAT64'),\n",
    " ('over_75perc_last_60_AVG', 'FLOAT64'),\n",
    " ('over_985perc_last_60', 'FLOAT64'),\n",
    " ('over_985perc_last_60_AVG', 'FLOAT64'),\n",
    " ('songs_last_120', 'FLOAT64'),\n",
    " ('songs_last_120_AVG', 'FLOAT64'),\n",
    " ('logins_last_120', 'FLOAT64'),\n",
    " ('logins_last_120_AVG', 'FLOAT64'),\n",
    " ('total_secs_last_120', 'FLOAT64'),\n",
    " ('total_secs_last_120_AVG', 'FLOAT64'),\n",
    " ('num_unq_last_120', 'FLOAT64'),\n",
    " ('num_unq_last_120_AVG', 'FLOAT64'),\n",
    " ('num_repeat_last_120', 'FLOAT64'),\n",
    " ('num_repeat_last_120_AVG', 'FLOAT64'),\n",
    " ('over_50perc_last_120', 'FLOAT64'),\n",
    " ('over_50perc_last_120_AVG', 'FLOAT64'),\n",
    " ('over_75perc_last_120', 'FLOAT64'),\n",
    " ('over_75perc_last_120_AVG', 'FLOAT64'),\n",
    " ('over_985perc_last_120', 'FLOAT64'),\n",
    " ('over_985perc_last_120_AVG', 'FLOAT64'),\n",
    " ('SUM_unq_songs_0_15', 'FLOAT64'),\n",
    " ('AVG_unq_songs_0_15', 'FLOAT64'),\n",
    " ('SUM_songs_0_15', 'FLOAT64'),\n",
    " ('AVG_songs_0_15', 'FLOAT64'),\n",
    " ('SUM_secs_0_15', 'FLOAT64'),\n",
    " ('AVG_secs_0_15', 'FLOAT64'),\n",
    " ('SUM_songs50_0_15', 'FLOAT64'),\n",
    " ('AVG_songs50_0_15', 'FLOAT64'),\n",
    " ('SUM_logins_0_15', 'FLOAT64'),\n",
    " ('AVG_logins_0_15', 'FLOAT64'),\n",
    " ('SUM_repeats_0_15', 'FLOAT64'),\n",
    " ('AVG_repeats_0_15', 'FLOAT64'),\n",
    " ('SUM_unq_songs_15_30', 'FLOAT64'),\n",
    " ('AVG_unq_songs_15_30', 'FLOAT64'),\n",
    " ('SUM_songs_15_30', 'FLOAT64'),\n",
    " ('AVG_songs_15_30', 'FLOAT64'),\n",
    " ('SUM_secs_15_30', 'FLOAT64'),\n",
    " ('AVG_secs_15_30', 'FLOAT64'),\n",
    " ('SUM_songs50_15_30', 'FLOAT64'),\n",
    " ('AVG_songs50_15_30', 'FLOAT64'),\n",
    " ('SUM_logins_15_30', 'FLOAT64'),\n",
    " ('AVG_logins_15_30', 'FLOAT64'),\n",
    " ('SUM_repeats_15_30', 'FLOAT64'),\n",
    " ('AVG_repeats_15_30', 'FLOAT64'),\n",
    " ('SUM_unq_songs_30_45', 'FLOAT64'),\n",
    " ('AVG_unq_songs_30_45', 'FLOAT64'),\n",
    " ('SUM_songs_30_45', 'FLOAT64'),\n",
    " ('AVG_songs_30_45', 'FLOAT64'),\n",
    " ('SUM_secs_30_45', 'FLOAT64'),\n",
    " ('AVG_secs_30_45', 'FLOAT64'),\n",
    " ('SUM_songs50_30_45', 'FLOAT64'),\n",
    " ('AVG_songs50_30_45', 'FLOAT64'),\n",
    " ('SUM_logins_30_45', 'FLOAT64'),\n",
    " ('AVG_logins_30_45', 'FLOAT64'),\n",
    " ('SUM_repeats_30_45', 'FLOAT64'),\n",
    " ('AVG_repeats_30_45', 'FLOAT64'),\n",
    " ('SUM_unq_songs_45_60', 'FLOAT64'),\n",
    " ('AVG_unq_songs_45_60', 'FLOAT64'),\n",
    " ('SUM_songs_45_60', 'FLOAT64'),\n",
    " ('AVG_songs_45_60', 'FLOAT64'),\n",
    " ('SUM_secs_45_60', 'FLOAT64'),\n",
    " ('AVG_secs_45_60', 'FLOAT64'),\n",
    " ('SUM_songs50_45_60', 'FLOAT64'),\n",
    " ('AVG_songs50_45_60', 'FLOAT64'),\n",
    " ('SUM_logins_45_60', 'FLOAT64'),\n",
    " ('AVG_logins_45_60', 'FLOAT64'),\n",
    " ('SUM_repeats_45_60', 'FLOAT64'),\n",
    " ('AVG_repeats_45_60', 'FLOAT64'),\n",
    " ('DIFSUM_unq_songs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFAVG_unq_songs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSUM_songs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFAVG_songs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSUM_secs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFAVG_secs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSUM_songs50_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFAVG_songs50_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSUM_logins_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFAVG_logins_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSUM_repeats_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFAVG_repeats_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSUM_unq_songs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFAVG_unq_songs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSUM_songs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFAVG_songs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSUM_secs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFAVG_secs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSUM_songs50_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFAVG_songs50_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSUM_logins_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFAVG_logins_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSUM_repeats_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFAVG_repeats_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSUM_unq_songs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFAVG_unq_songs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSUM_songs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFAVG_songs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSUM_secs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFAVG_secs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSUM_songs50_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFAVG_songs50_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSUM_logins_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFAVG_logins_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSUM_repeats_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFAVG_repeats_30_45_45_60', 'FLOAT64'),\n",
    " ('expire_last_login', 'INT64'),\n",
    " ('total_cancelations', 'INT64'),\n",
    " ('login_after_expire_10', 'INT64'),\n",
    " ('login_after_expire_20', 'INT64'),\n",
    " ('login_after_expire_30', 'INT64'),\n",
    " ('STD_unq_songs_0_15', 'FLOAT64'),\n",
    " ('STD_songs_0_15', 'FLOAT64'),\n",
    " ('STD_secs_0_15', 'FLOAT64'),\n",
    " ('STD_songs50_0_15', 'FLOAT64'),\n",
    " ('STD_repeats_0_15', 'FLOAT64'),\n",
    " ('STD_unq_songs_15_30', 'FLOAT64'),\n",
    " ('STD_songs_15_30', 'FLOAT64'),\n",
    " ('STD_secs_15_30', 'FLOAT64'),\n",
    " ('STD_songs50_15_30', 'FLOAT64'),\n",
    " ('STD_repeats_15_30', 'FLOAT64'),\n",
    " ('STD_unq_songs_30_45', 'FLOAT64'),\n",
    " ('STD_songs_30_45', 'FLOAT64'),\n",
    " ('STD_secs_30_45', 'FLOAT64'),\n",
    " ('STD_songs50_30_45', 'FLOAT64'),\n",
    " ('STD_repeats_30_45', 'FLOAT64'),\n",
    " ('STD_unq_songs_45_60', 'FLOAT64'),\n",
    " ('STD_songs_45_60', 'FLOAT64'),\n",
    " ('STD_secs_45_60', 'FLOAT64'),\n",
    " ('STD_songs50_45_60', 'FLOAT64'),\n",
    " ('STD_repeats_45_60', 'FLOAT64'),\n",
    " ('DIFSTD_unq_songs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSTD_songs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSTD_secs_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSTD_songs50_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSTD_repeats_0_15_15_30', 'FLOAT64'),\n",
    " ('DIFSTD_unq_songs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSTD_songs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSTD_secs_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSTD_songs50_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSTD_repeats_15_30_30_45', 'FLOAT64'),\n",
    " ('DIFSTD_unq_songs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSTD_songs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSTD_secs_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSTD_songs50_30_45_45_60', 'FLOAT64'),\n",
    " ('DIFSTD_repeats_30_45_45_60', 'FLOAT64'),\n",
    " ('is_cancel', 'INT64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Correctly Cast DRV_Feb2016\n",
    "for feature, datatype in column_types_pd:\n",
    "    if datatype == 'STRING':\n",
    "        exec(f'DRV_Jan2016_1to1 = DRV_Feb2016.withColumn(\"{feature}\", expr(\"CAST({feature} AS string)\"))')\n",
    "    if datatype == 'DATE':\n",
    "        exec(f'DRV_Jan2016_1to1 = DRV_Feb2016.withColumn(\"{feature}\", expr(\"CAST({feature} AS timestamp)\"))')\n",
    "    if datatype == 'INT64':\n",
    "        exec(f'DRV_Jan2016_1to1 = DRV_Feb2016.withColumn(\"{feature}\", expr(\"CAST({feature} AS integer)\"))')\n",
    "    if datatype == 'FLOAT64':\n",
    "        exec(f'DRV_Jan2016_1to1 = DRV_Feb2016.withColumn(\"{feature}\", expr(\"CAST({feature} AS double)\"))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pre-Processing\n",
    "https://medium.com/@dhiraj.p.rai/essentials-of-feature-engineering-in-pyspark-part-i-76a57680a85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Split Feautres by Categorical or Continuous</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of Categorical feature names\n",
    "cat_feats = ['is_auto_renew', 'total_spent_zero', 'city_agg', 'payment_method_agg', 'never_active_subscriber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of Continuous feature names\n",
    "cont_feats = [x for x in DRV_Jan2016.columns if x not in cat_feats]\n",
    "cont_feats.remove('msno')\n",
    "cont_feats.remove('is_churn')\n",
    "cont_feats.remove('membership_expire_date')\n",
    "cont_feats.remove('registration_init_time')\n",
    "cont_feats.remove('city')\n",
    "cont_feats.remove('bd')\n",
    "cont_feats.remove('payment_method_id')\n",
    "cont_feats.remove('is_net_paid_amount')\n",
    "cont_feats.remove('registered_via')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Data Pre-Processing</font> -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>*Vector Assembler*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create master list of feature names for model\n",
    "final_features = cont_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the numeric features we will be transforming, and the name of the resulting output feature\n",
    "assembler = VectorAssembler(\n",
    "    inputCols= final_features,\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>*Feature Scaling*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all features into our final output features\n",
    "scaler = StandardScaler(inputCol='features', \n",
    "                        outputCol='features_scaled',\n",
    "                        withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation: Pipeline and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Create Pipeline Object</font> -\n",
    "https://spark.apache.org/docs/2.4.3/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = DRV_Jan2016.limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-82056110792f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Calculate silhouette scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0msilhouette_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mch_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalinski_harabasz_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"For n_clusters =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    221\u001b[0m                                     labels=labels, label_freqs=label_freqs)\n\u001b[0;32m    222\u001b[0m     results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func,\n\u001b[1;32m--> 223\u001b[1;33m                                               **kwds))\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minter_clust_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mintra_clust_dists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintra_clust_dists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1446\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[1;32m-> 1448\u001b[1;33m                                      n_jobs=n_jobs, **kwds)\n\u001b[0m\u001b[0;32m   1449\u001b[0m         if ((X is Y or Y is None)\n\u001b[0;32m   1450\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1586\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "# for i, n_clusters in enumerate([2]):\n",
    "\n",
    "i = 0\n",
    "n_clusters = 2\n",
    "\n",
    "## Cluster Calculation ##\n",
    "print('1')\n",
    "# Initialize the clusterer with n_clusters value and a random generator seed of 10 for reproducibility.\n",
    "kmeans = KMeans(featuresCol='features_scaled',\n",
    "                initMode='k-means||').setK(n_clusters).setSeed(11)\n",
    "\n",
    "# Create pipeline object\n",
    "kmeans = Pipeline(stages=[assembler,scaler,kmeans])\n",
    "print('2')\n",
    "\n",
    "## Cluster Evaluation ##\n",
    "\n",
    "# Fit to DRV_January\n",
    "model = kmeans.fit(DRV_Jan2016)\n",
    "print('3')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(DRV_Jan2016)\n",
    "print('4')\n",
    "\n",
    "# Make Cluster Labels\n",
    "cluster_labels = predictions.select('prediction').toPandas().prediction\n",
    "print('5')\n",
    "\n",
    "# Calculate silhouette scores\n",
    "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "ch_score = metrics.calinski_harabasz_score(X_scaled, cluster_labels)  \n",
    "print(\"For n_clusters =\", n_clusters)\n",
    "print(\"The average silhouette_score is :\", silhouette_avg)\n",
    "print(\"The calinski_harabasz_score is :\", ch_score)\n",
    "print('')\n",
    "print('6')\n",
    "# Calculate silhouette scores\n",
    "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "ch_score = metrics.calinski_harabasz_score(X_scaled, cluster_labels)  \n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)\n",
    "print('7')\n",
    "## Cluster Visualization ##\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "plt.subplot(3,2,i+1)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1 but in this example all\n",
    "# lie within [-0.1, 1]\n",
    "plt.xlim([-0.1, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "plt.ylim([0, len(X_scaled) + (n_clusters + 1) * 10])\n",
    "print('8')\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "plt.title(\"The silhouette plot for the various clusters.\")\n",
    "plt.xlabel(\"The silhouette coefficient values\")\n",
    "plt.ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "plt.xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = predictions.select('prediction').toPandas().prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.627515869783927"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
