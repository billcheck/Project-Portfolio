{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKBox Customer Churn - Supervised Learning Capstone - Data Loading Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Competition: https://www.kaggle.com/c/kkbox-churn-prediction-challenge/data\n",
    "\n",
    "**Scope:** I am trying to see if a user who is active in February 2017, will still be a user in March 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preview Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Import Libraries</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import missingno as msno\n",
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Summary of Datasets</font> -\n",
    "For this project we are given several massive datasets totaling over 30 GB. In general the datasets are divided into two versions: ***v1*** and ***v2***.\n",
    "_____\n",
    "**train_v1:** containing the user ids and whether they churned until ***2/28/2017***.\n",
    "\n",
    "**train_v2:** containing the user ids and whether they churned for the month of ***March 2017***.\n",
    "\n",
    "Features:\n",
    "    - msno: user id\n",
    "    - is_churn: This is the target variable. Churn is defined as whether the user did not continue the subscription within 30 days of expiration. is_churn = 1 means churn,is_churn = 0 means renewal.\n",
    "\n",
    "_____\n",
    "**transactions_v1:** transactions of users up until ***2/28/2017***.\n",
    "\n",
    "**transactions_v2:** transactions of users up until ***3/31/2017***.\n",
    "\n",
    "Features:\n",
    "    - msno: user id (***Repeated as a user can have various Transactions***)\n",
    "    - payment_method_id: payment method\n",
    "    - payment_plan_days: length of membership plan in days\n",
    "    - plan_list_price: in New Taiwan Dollar (NTD)\n",
    "    - actual_amount_paid: in New Taiwan Dollar (NTD)\n",
    "    - is_auto_renew\n",
    "    - transaction_date: format %Y%m%d\n",
    "    - membership_expire_date: format %Y%m%d\n",
    "    - is_cancel: whether or not the user canceled the membership in this transaction.\n",
    "\n",
    "_____\n",
    "**user_log_v1:** transactions of users up until ***2/28/2017***.\n",
    "\n",
    "**user_log_v2:** transactions of users for the month of ***March 2017***.\n",
    "\n",
    "Features:\n",
    "    - msno: user id (***Repeated as a user can have various Logins***)\n",
    "    - date: format %Y%m%d\n",
    "    - num_25: # of songs played less than 25% of the song length\n",
    "    - num_50: # of songs played between 25% to 50% of the song length\n",
    "    - num_75: # of songs played between 50% to 75% of of the song length\n",
    "    - num_985: # of songs played between 75% to 98.5% of the song length\n",
    "    - num_100: # of songs played over 98.5% of the song length\n",
    "    - num_unq: # of unique songs played\n",
    "    - total_secs: total seconds played\n",
    "\n",
    "_____\n",
    "**members_v3:** All user information data.\n",
    "\n",
    "Features:\n",
    "    - msno: user id\n",
    "    - city\n",
    "    - bd: age. Note: this column has outlier values ranging from -7000 to 2015, please use your judgement.\n",
    "    - gender\n",
    "    - registered_via: registration method\n",
    "    - registration_init_time: format %Y%m%d\n",
    "\n",
    "_____\n",
    "\n",
    "#### - <font color=blue>Dataset Statistics</font> -\n",
    "**# of Observations**: > 300,000,000\n",
    "\n",
    "**Dataset Sizes**\n",
    "\n",
    "- ***train_v1 Dataset:*** 45.56 MB \n",
    "- ***train_v2 Dataset:*** 44.56 MB   \n",
    "- ***transactions_v1 Dataset:*** 1.68 GB     \n",
    "- ***transactions_v2 Dataset:*** 112.69 MB     \n",
    "- ***user_log_v1 Dataset:*** 29.78 GB     \n",
    "- ***user_log_v2 Dataset:*** 1.40 GB \n",
    "- ***members_v3 Dataset:*** 417.89 MB\n",
    "- ***<font color=red>Total:  33.48 GB</font>***\n",
    "\n",
    "\n",
    "For the most part, ***January 2017 - Train Set*** , ***February 2017 - Validation Set*** data will be coming from the ***v1*** files and ***March 2017 - Test Set*** data will be coming from the ***v2*** files. Although the initial sets will be somewhat limited in features, once we have them imported we will make various queries to create new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Import Data into Database</font> -\n",
    "As described in the 'KKBox Data Loading Methods' Notebook, after working out a succefully lengthy solution completely in Python, I decided to utilize SQLite3 for a more 'Persistent' solution. Postgre was all so considered but this was more practical for the task.\n",
    "\n",
    "Since the data in each dataset are in different timeframes, the initial Train, Validation, and Test Sets will contain general information for each member. For example:\n",
    "- The Transaction datset has recorded every single transaction made by a user.\n",
    "- The User Log dataset has recorded every single time a user has logged onto the platform.\n",
    "\n",
    "Since these datasets capture different types of user behaviors over different timeframes we can't just join them all together immediately. However since they do capture behavior over time, I believe that there would be a ton of value if we are able to get creative on how we capture this ***retrospective data***. As we go through EDA and Feature Creation we will create these new features through additional queries and python commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Connection to SQLite\n",
    "cnx = sql.connect(\"C:\\J-5 Local SSD\\Projects\\KKBox Customer Churn\\Database\\KKBox_DB.db\")  # Opens file if exists, else creates file\n",
    "cur = cnx.cursor()  # This object lets us actually send messages to our DB and receive results\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set file path for all Data\n",
    "# path = 'C:/J-5 Local SSD/Projects/KKBox Customer Churn/Datasets/'\n",
    "\n",
    "# # Create list of all dataset names\n",
    "# data_list = ['train_v1', 'train_v2', \"transactions_v1\", 'transactions_v2', 'user_logs_v1', 'user_logs_v2', 'members_v3']\n",
    "\n",
    "# for dset in data_list:\n",
    "#     for chunk in pd.read_csv(path+dset+'.csv', chunksize=1000000):\n",
    "#         chunk.to_sql(name=dset, con=cnx, if_exists=\"append\", index=False)  #\"name\" is name of table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our datasets are loaded, we will create our Train, Validation, and Test sets respective to their indiviudal months and write them as ***train_jan, valid_feb, test_march.***\n",
    "    \n",
    "By creating these sets in our SQLite Database we avoid having to do this in-memory here in this JupyterNB. Also, this is mainly done to simulate working with a massive dataset stored in a database as is expected to happen in practice. Not toy datasets here :) \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Build out our Datasets</font> -\n",
    "\n",
    "**How we will build them:**\n",
    "    \n",
    "    1) Get all unique users from transactions v1 and v2 with expirations dates falling within each timeframe.\n",
    "       - Now that we have the basis for each set we will merge continuous merge on msno for each set.\n",
    "\n",
    "    2) Join members_v3 and all sets on msno\n",
    "\n",
    "    3.a) Join train_v1 and train_jan on msno\n",
    "\n",
    "    3.b) Join train_v1 and val_feb on msno \n",
    "\n",
    "    4) Join train_v2 and test_mar on msno\n",
    "\n",
    "This will be all done using SQL commands from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build all the basis for all three Main Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all three sets with respect to their respective months and all the users who have Memebership Expirations falling in those months\n",
    "sets = {'train_jan': ['transactions_v1','20170101','20170101'], 'valid_feb': ['transactions_v1','20170201','20170228'], 'test_mar': ['transactions_v2','20170301','20170331']}\n",
    "\n",
    "for setname, info in sets.items():  \n",
    "    cur.execute(f'''CREATE TABLE IF NOT EXISTS {setname} AS\n",
    "                    SELECT *\n",
    "                    FROM {info[0]}\n",
    "                    WHERE membership_expire_date >= {info[1]} AND membership_expire_date <= {info[2]}\n",
    "    ''')\n",
    "    \n",
    "cnx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all table names in DB\n",
    "alltable_names = [name[0] for name in cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")]\n",
    "\n",
    "# Create a list of all dataset names\n",
    "datasets = alltable_names[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index msno columns on all tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_v1 index created!\n",
      "train_v2 index created!\n",
      "transactions_v1 index created!\n",
      "transactions_v2 index created!\n",
      "user_logs_v1 index created!\n",
      "user_logs_v2 index created!\n",
      "members_v3 index created!\n",
      "train_jan index created!\n",
      "valid_feb index created!\n",
      "test_mar index created!\n"
     ]
    }
   ],
   "source": [
    "# Index msno columns in all tables, to help with performance moving forward\n",
    "for table in alltable_names:\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS msno_idx ON {table}(msno);\")\n",
    "    cnx.commit\n",
    "    print(f'{table} index created!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join members_v3 data onto Main Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of member_v3 columns\n",
    "member_columns = [column[0] for column in cur.execute('select * from members_v3 limit 10').description]\n",
    "member_columns.remove('msno')\n",
    "\n",
    "# Create dictionary of member_columns and their datatypes\n",
    "datatypes = ['INTEGER','INTEGER','TEXT','INTEGER','INTEGER']\n",
    "member_dict = dict(zip(member_columns, datatypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_jan Columns have been added\n",
      "valid_feb Columns have been added\n",
      "test_mar Columns have been added\n"
     ]
    }
   ],
   "source": [
    "# Create new columns in each Main Dataset to add members_v3 data\n",
    "for dataset in datasets:\n",
    "    for column, coltype in member_dict.items():\n",
    "        cur.execute(f'ALTER TABLE {dataset} ADD COLUMN {column} {coltype};')\n",
    "        cnx.commit()\n",
    "        \n",
    "    print(f'{dataset} Columns have been added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join members_v3 and all sets on msno\n",
    "for dataset in datasets:\n",
    "    for column, coltype in member_dict.items():\n",
    "        cur.execute(f'UPDATE {dataset} SET {column} = (SELECT {column} FROM members_v3 WHERE msno = {dataset}.msno)')\n",
    "        cnx.commit()\n",
    "    \n",
    "    print(f'{dataset} Has joined successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join train_v1 and train_v2 onto all Main Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of train_v1/train_v2 columns\n",
    "train_columns = [column[0] for column in cur.execute('select * from train_v1 limit 1').description]\n",
    "train_columns.remove('msno')\n",
    "\n",
    "# Create dictionary of member_columns and their datatypes\n",
    "datatypes = ['INTEGER']\n",
    "train_dict = dict(zip(train_columns, datatypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns in each Main Dataset to add train_v1/train_v2 data\n",
    "for dataset in datasets:\n",
    "    for column, coltype in train_dict.items():\n",
    "        cur.execute(f'ALTER TABLE {dataset} ADD COLUMN {column} {coltype};')\n",
    "        cnx.commit()\n",
    "        \n",
    "    print(f'{dataset} Columns have been added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join train_v1 and train_jan,valid_feb on msno\n",
    "for dataset in datasets[:2]:\n",
    "    for column, coltype in train_dict.items():\n",
    "        cur.execute(f'UPDATE {dataset} SET {column} = (SELECT {column} FROM train_v1 WHERE msno = {dataset}.msno)')\n",
    "        cnx.commit()\n",
    "    \n",
    "    print(f'{dataset} Has joined successfully')\n",
    "\n",
    "# Join train_v2 and test_mmar on msno\n",
    "for dataset in datasets[2]:\n",
    "    for column, coltype in train_dict.items():\n",
    "        cur.execute(f'UPDATE {dataset} SET {column} = (SELECT {column} FROM train_v2 WHERE msno = {dataset}.msno)')\n",
    "        cnx.commit()\n",
    "    \n",
    "    print(f'{dataset} Has joined successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Pull Training Data from Database</font> -\n",
    "Our Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Connection to SQLite\n",
    "df = pd.read_sql_query(\"SELECT * FROM train_jan\", cnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Detect Missing Values in Dataset</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Train</th>\n",
       "      <th>Percent Train</th>\n",
       "      <th>Total Test</th>\n",
       "      <th>Percent Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>601525</td>\n",
       "      <td>0.605428</td>\n",
       "      <td>582129</td>\n",
       "      <td>0.599324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_25_sum</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_50_sum</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_count</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_secs_sum</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_unq_sum</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_100_sum</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_985_sum</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_75_sum</th>\n",
       "      <td>229963</td>\n",
       "      <td>0.231455</td>\n",
       "      <td>216427</td>\n",
       "      <td>0.222820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bd</th>\n",
       "      <td>115800</td>\n",
       "      <td>0.116551</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.113249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>115800</td>\n",
       "      <td>0.116551</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.113249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>registration_init_time</th>\n",
       "      <td>115800</td>\n",
       "      <td>0.116551</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.113249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>registered_via</th>\n",
       "      <td>115800</td>\n",
       "      <td>0.116551</td>\n",
       "      <td>110000</td>\n",
       "      <td>0.113249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_cancel</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>membership_expire_date</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transaction_date</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_auto_renew</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_amount_paid</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plan_list_price</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_plan_days</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_method_id</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37382</td>\n",
       "      <td>0.038486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_churn</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msno</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Total Train  Percent Train  Total Test  Percent Test\n",
       "gender                       601525       0.605428      582129      0.599324\n",
       "num_25_sum                   229963       0.231455      216427      0.222820\n",
       "num_50_sum                   229963       0.231455      216427      0.222820\n",
       "date_count                   229963       0.231455      216427      0.222820\n",
       "total_secs_sum               229963       0.231455      216427      0.222820\n",
       "num_unq_sum                  229963       0.231455      216427      0.222820\n",
       "num_100_sum                  229963       0.231455      216427      0.222820\n",
       "num_985_sum                  229963       0.231455      216427      0.222820\n",
       "num_75_sum                   229963       0.231455      216427      0.222820\n",
       "bd                           115800       0.116551      110000      0.113249\n",
       "city                         115800       0.116551      110000      0.113249\n",
       "registration_init_time       115800       0.116551      110000      0.113249\n",
       "registered_via               115800       0.116551      110000      0.113249\n",
       "is_cancel                         0       0.000000       37382      0.038486\n",
       "membership_expire_date            0       0.000000       37382      0.038486\n",
       "transaction_date                  0       0.000000       37382      0.038486\n",
       "is_auto_renew                     0       0.000000       37382      0.038486\n",
       "actual_amount_paid                0       0.000000       37382      0.038486\n",
       "plan_list_price                   0       0.000000       37382      0.038486\n",
       "payment_plan_days                 0       0.000000       37382      0.038486\n",
       "payment_method_id                 0       0.000000       37382      0.038486\n",
       "is_churn                          0       0.000000           0      0.000000\n",
       "msno                              0       0.000000           0      0.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table of Features and their respective totals percentages of missing data in both Test and Train sets\n",
    "total_missing_train = train_jan.isnull().sum().sort_values(ascending=False)\n",
    "percent_missing_train = (train_jan.isnull().sum()/train_jan.isnull().count())\n",
    "\n",
    "total_missing_valid = valid_feb.isnull().sum().sort_values(ascending=False)\n",
    "percent_missing_valid = (valid_feb.isnull().sum()/valid_feb.isnull().count())\n",
    "\n",
    "total_missing_test = test_mar.isnull().sum().sort_values(ascending=False)\n",
    "percent_missing_test = (test_mar.isnull().sum()/test_mar.isnull().count())\n",
    "\n",
    "columns = [total_missing_train, percent_missing_train, total_missing_valid, percent_missing_valid, total_missing_test, percent_missing_test]\n",
    "\n",
    "missing_data = pd.concat(columns, axis=1, keys=['Total Missing Train', 'Percent Missing Train', 'Total Missing Validation', 'Percent Missing Validation', 'Total Missing Test', 'Percent Missing Test'], sort=False).sort_values(by='Percent Train',ascending = False)\n",
    "missing_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
