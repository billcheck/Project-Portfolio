{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKBox Customer Churn Prediction\n",
    "### w/ BigQuery and Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: <font color=green>*Extraction, Transformation, and Loading*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Imports for BigQuery connection\n",
    "import json\n",
    "import pprint\n",
    "import subprocess\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Import Data into Database</font> -\n",
    "Given such a large dataset, I decided it would be best to work with a cloud-based DBMS coupled with Apache Spark for this project.\n",
    "\n",
    "Since the data in each dataset are in different timeframes, the initial Train, Validation, and Test Sets will contain general information for each member. For example:\n",
    "- The Transaction datset has recorded every single transaction made by a user.\n",
    "- The User Log dataset has recorded every single time a user has logged onto the platform.\n",
    "\n",
    "Since these datasets capture different types of user behaviors over different timeframes we can't just join them all together immediately. However since they do capture behavior over time, I believe that there would be a ton of value if we are able to get creative on how we capture this ***retrospective data***. As we go through EDA and Feature Creation we will create these new features through additional queries and python commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Summary of Datasets</font> -\n",
    "For this project we are given several massive datasets totaling over 30 GB. In general the datasets are divided into two versions: ***v1*** and ***v2***. We will only be using ***v1*** files as since they contain nearly 3 years worth of data and ***v2*** only contains a single month.\n",
    "_____\n",
    "**train_v1:** containing the user ids and whether they churned until ***2/28/2017***.\n",
    "\n",
    "Features:\n",
    "\n",
    "    - msno: user id\n",
    "    - is_churn: This is the target variable. Churn is defined as whether the user did not continue the subscription within 30 days of expiration. is_churn = 1 means churn,is_churn = 0 means renewal.\n",
    "\n",
    "_____\n",
    "**transactions_v1:** transactions of users up until ***2/28/2017***.\n",
    "\n",
    "Features:\n",
    "\n",
    "    - msno: user id (***Repeated as a user can have various Transactions***)\n",
    "    - payment_method_id: payment method\n",
    "    - payment_plan_days: length of membership plan in days\n",
    "    - plan_list_price: in New Taiwan Dollar (NTD)\n",
    "    - actual_amount_paid: in New Taiwan Dollar (NTD)\n",
    "    - is_auto_renew\n",
    "    - transaction_date: format %Y%m%d\n",
    "    - membership_expire_date: format %Y%m%d\n",
    "    - is_cancel: whether or not the user canceled the membership in this transaction.\n",
    "\n",
    "_____\n",
    "**user_log_v1:** transactions of users up until ***2/28/2017***.\n",
    "\n",
    "Features:\n",
    "\n",
    "    - msno: user id (***Repeated as a user can have various Logins***)\n",
    "    - date: format %Y%m%d\n",
    "    - num_25: # of songs played less than 25% of the song length\n",
    "    - num_50: # of songs played between 25% to 50% of the song length\n",
    "    - num_75: # of songs played between 50% to 75% of of the song length\n",
    "    - num_985: # of songs played between 75% to 98.5% of the song length\n",
    "    - num_100: # of songs played over 98.5% of the song length\n",
    "    - num_unq: # of unique songs played\n",
    "    - total_secs: total seconds played\n",
    "\n",
    "_____\n",
    "**members_v3:** All user information data.\n",
    "\n",
    "Features:\n",
    "\n",
    "    - msno: user id\n",
    "    - city\n",
    "    - bd: age. Note: this column has outlier values ranging from -7000 to 2015, please use your judgement.\n",
    "    - gender\n",
    "    - registered_via: registration method\n",
    "    - registration_init_time: format %Y%m%d\n",
    "\n",
    "_____\n",
    "\n",
    "#### - <font color=blue>Dataset Statistics</font> -\n",
    "\n",
    "- ***train_v1 Dataset:*** ~800K Records @ 45.56 MB \n",
    "- ***transactions_v1 Dataset:*** ~22 Million Records @ 1.68 GB         \n",
    "- ***user_log_v1 Dataset:*** ~400 Million Records @ 29.78 GB     \n",
    "- ***members_v3 Dataset:*** ~5 Million Records @ 417.89 MB\n",
    "- **<font color=red>Total:  31.92 GB</font>**  w/ 22 raw data points across all files, including 4 date fields\n",
    "\n",
    "Each of the datasets will be imported into Google BigQuery as Raw *RAW_* Tables. These Raw Tables will always remain untouched and will be used as a clean backup if we were to make a mistake in any of our transformations in the future. We will then import these datasets again but as Working Tables. These Working Tables are what we will use to perform all cleaning and data prep as they will provide a safe working environment away from the source datasets. When we get into the feature engineering phase of the project, we will then create a third type of dataset called Derived Tables. These tables will be used for all modeling purposes. I find that creating these 3 different table types allows for a clean and organized workflow. To summarize:\n",
    "- **Raw Tables (*RAW_*)** - Raw untouched verison of all tables\n",
    "- **Working Tables (*WRK_*)** - Cleaned and properly formatted version of all tables. These will serve as the source for our Derived Tables.\n",
    "- **Derived Tables (*DRV_*)** - Table created specifically for our use case's model. All feature engineering will be performed here.\n",
    "\n",
    "Given such a large dataset, I decided it would be best to work with a cloud-based Database Management System DBMS (Google Bigquery) coupled with Apache Spark (for distributed processing) for this project. Due to the size of this dataset, and the large amount of feature engineering I will inevitably be performing, a cloud-based DBMS will provide for me a stable environment for storing data, transformations, and new features. Small Disclaimer: I know Google BigQuery isn't intended to be used as a DBMS, and something like a Postgre and MySQL Server would be more appropriate for this use case, but... they offered to let me use it for free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to BigQuery and Preview Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Connect to BigQuery</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Specify Google Credentials\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='D:\\OneDrive\\J-5\\GitHub\\Google Credentials.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BigQuery extension\n",
    "bigquery_client = bigquery.Client(project='spark-kkbox')\n",
    "\n",
    "# Instantiate BigQuery magic\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Create *Working (WRK)* Tables from our Raw Tables</font> -\n",
    "Here we will simply make copies of each of our Raw Tables. We will perform all cleaning operations on these copies. Here is also where we will modify column types if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***User Logs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery user_logs\n",
    "CREATE TABLE `spark-kkbox.KKbox_User_Data.WRK_users_logs` AS\n",
    "SELECT \n",
    "    * EXCEPT(date),\n",
    "    PARSE_DATE('%Y%m%d', CAST(date AS STRING)) AS date\n",
    "FROM `spark-kkbox.KKbox_User_Data.RAW_user_logs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transactions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery user_logs\n",
    "CREATE TABLE `spark-kkbox.KKbox_User_Data.WRK_transactions_v1` AS\n",
    "SELECT\n",
    "    * EXCEPT(transaction_date, membership_expire_date),\n",
    "    PARSE_DATE('%Y%m%d', CAST(transaction_date AS STRING)) AS transaction_date,\n",
    "    PARSE_DATE('%Y%m%d', CAST(membership_expire_date AS STRING)) AS membership_expire_date\n",
    "FROM `spark-kkbox.KKbox_User_Data.RAW_transactions_v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Members***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery user_logs\n",
    "CREATE TABLE `spark-kkbox.KKbox_User_Data.WRK_members_v3` AS\n",
    "SELECT\n",
    "    * EXCEPT(registration_init_time),\n",
    "    PARSE_DATE('%Y%m%d', CAST(registration_init_time AS STRING)) AS registration_init_time\n",
    "FROM `spark-kkbox.KKbox_User_Data.RAW_members_v3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Member Churn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery user_logs\n",
    "CREATE TABLE `spark-kkbox.KKbox_User_Data.WRK_train_v1` AS\n",
    "SELECT *\n",
    "FROM `spark-kkbox.KKbox_User_Data.RAW_train_v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Preview Data Tables</font> -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***User Logs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery user_logs\n",
    "SELECT *\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_users_logs`\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>num_25</th>\n",
       "      <th>num_50</th>\n",
       "      <th>num_75</th>\n",
       "      <th>num_985</th>\n",
       "      <th>num_100</th>\n",
       "      <th>num_unq</th>\n",
       "      <th>total_secs</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>q8B6uGiI4d9g0d6XRH/E2en0SZbb6JLbOlolugXlBs8=</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>101</td>\n",
       "      <td>77</td>\n",
       "      <td>28397.521</td>\n",
       "      <td>2016-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5/srE+iaqjfCuuJlb0SlWvFtVFY7QLB+gu96nZdhv4Y=</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>25711.340</td>\n",
       "      <td>2017-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>aDhcEfcLIhA+2iY49Ujk8oOYQ1MwYtPJHegHZfvnVW8=</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>173</td>\n",
       "      <td>169</td>\n",
       "      <td>45653.880</td>\n",
       "      <td>2015-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>hHVAexGE2Om5EO6PyyWgblX0ij3ORgSpy9Oz1F4mcso=</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>161</td>\n",
       "      <td>132</td>\n",
       "      <td>43869.192</td>\n",
       "      <td>2015-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AHHiMP9YU0g3oYqJKvaHQkdWZVVVJhbbM0uqJQxG3eE=</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>1157.172</td>\n",
       "      <td>2016-11-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  num_25  num_50  num_75  \\\n",
       "0  q8B6uGiI4d9g0d6XRH/E2en0SZbb6JLbOlolugXlBs8=      23       3       4   \n",
       "1  5/srE+iaqjfCuuJlb0SlWvFtVFY7QLB+gu96nZdhv4Y=      27       6       3   \n",
       "2  aDhcEfcLIhA+2iY49Ujk8oOYQ1MwYtPJHegHZfvnVW8=       0       2       1   \n",
       "3  hHVAexGE2Om5EO6PyyWgblX0ij3ORgSpy9Oz1F4mcso=      55      23      14   \n",
       "4  AHHiMP9YU0g3oYqJKvaHQkdWZVVVJhbbM0uqJQxG3eE=      60       5       0   \n",
       "\n",
       "   num_985  num_100  num_unq  total_secs        date  \n",
       "0        8      101       77   28397.521  2016-01-19  \n",
       "1       10       70       85   25711.340  2017-01-30  \n",
       "2        6      173      169   45653.880  2015-06-01  \n",
       "3        5      161      132   43869.192  2015-03-10  \n",
       "4        3        2       67    1157.172  2016-11-21  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Transactions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery transactions\n",
    "SELECT *\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_transactions_v1`\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_plan_days</th>\n",
       "      <th>plan_list_price</th>\n",
       "      <th>actual_amount_paid</th>\n",
       "      <th>is_auto_renew</th>\n",
       "      <th>is_cancel</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>membership_expire_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tfJbPZLIgve/BgVWchKztOHDmWQGd2DrG6wnSZzdtXo=</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>2015-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nWQxBW6uZrEqf+h/il1GaSMbIRtkmpliBOvRggaN41U=</td>\n",
       "      <td>34</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>2016-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>BoAUrW/VZiY7jGNFklcSJtYpYMTmpWqfZdxilj4EIA8=</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-21</td>\n",
       "      <td>2016-07-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7gexlt6niM4X4hADxKPsMmfFkkwHZpbsBMiQN5TfqjE=</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>jxZc8KGiYRqqI3uY1xezrYPt99MjMQO3PKW5s9CRKdU=</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-10</td>\n",
       "      <td>2015-12-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  payment_method_id  \\\n",
       "0  tfJbPZLIgve/BgVWchKztOHDmWQGd2DrG6wnSZzdtXo=                 38   \n",
       "1  nWQxBW6uZrEqf+h/il1GaSMbIRtkmpliBOvRggaN41U=                 34   \n",
       "2  BoAUrW/VZiY7jGNFklcSJtYpYMTmpWqfZdxilj4EIA8=                 36   \n",
       "3  7gexlt6niM4X4hADxKPsMmfFkkwHZpbsBMiQN5TfqjE=                 40   \n",
       "4  jxZc8KGiYRqqI3uY1xezrYPt99MjMQO3PKW5s9CRKdU=                 40   \n",
       "\n",
       "   payment_plan_days  plan_list_price  actual_amount_paid  is_auto_renew  \\\n",
       "0                 30              149                 149              0   \n",
       "1                 30              149                 149              1   \n",
       "2                 30              180                 180              1   \n",
       "3                 30              149                 149              1   \n",
       "4                 30              149                 149              1   \n",
       "\n",
       "   is_cancel transaction_date membership_expire_date  \n",
       "0          0       2015-11-01             2015-12-01  \n",
       "1          0       2016-07-31             2016-08-31  \n",
       "2          0       2016-06-21             2016-07-20  \n",
       "3          0       2016-10-01             2016-11-06  \n",
       "4          0       2015-11-10             2015-12-09  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Members***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery members\n",
    "SELECT *\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_members_v3`\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>registration_init_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>GLKNIdSePsWHv+jIwIeUzc+Rg6fO3hrYQD3yP604ZU4=</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>13</td>\n",
       "      <td>2016-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>R1QlT+WwTH9Ge41mt5OWXEAOZZG6abt6NVZ4yeTE4lU=</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>P/Jw4MNLvfODOLBMXnuprsWoTDk2Tvez9k9uYPUDOH4=</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>V2Hq2HVE85FJg3ar8+7T3ZnLdVqz3vjkiV39whQz9rA=</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>OHmLdcqEu7aYKZLP9IoM8dTLYI/RK84Dlk4CWA+Ttn4=</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2012-11-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  city  bd gender  \\\n",
       "0  GLKNIdSePsWHv+jIwIeUzc+Rg6fO3hrYQD3yP604ZU4=    10   0   None   \n",
       "1  R1QlT+WwTH9Ge41mt5OWXEAOZZG6abt6NVZ4yeTE4lU=     7   0   None   \n",
       "2  P/Jw4MNLvfODOLBMXnuprsWoTDk2Tvez9k9uYPUDOH4=    20   0   None   \n",
       "3  V2Hq2HVE85FJg3ar8+7T3ZnLdVqz3vjkiV39whQz9rA=     7   0   None   \n",
       "4  OHmLdcqEu7aYKZLP9IoM8dTLYI/RK84Dlk4CWA+Ttn4=     7   0   None   \n",
       "\n",
       "   registered_via registration_init_time  \n",
       "0              13             2016-11-03  \n",
       "1               3             2013-02-07  \n",
       "2               3             2013-03-31  \n",
       "3               3             2013-09-22  \n",
       "4               3             2012-11-29  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Member Churn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery churn\n",
    "SELECT *\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_train_v1`\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>waLDQMmcOu2jLDaV1ddDkgCrB/jl6sD66Xzs0Vqax1Y=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>QA7uiXy8vIbUSPOkCf9RwQ3FsT8jVq2OxDr8zqa7bRQ=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fGwBva6hikQmTJzrbz/2Ezjm5Cth5jZUNvXigKK2AFA=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mT5V8rEpa+8wuqi6x0DoVd3H5icMKkE9Prt49UlmK+4=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>XaPhtGLk/5UvvOYHcONTwsnH97P4eGECeq+BARGItRw=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno  is_churn\n",
       "0  waLDQMmcOu2jLDaV1ddDkgCrB/jl6sD66Xzs0Vqax1Y=         1\n",
       "1  QA7uiXy8vIbUSPOkCf9RwQ3FsT8jVq2OxDr8zqa7bRQ=         1\n",
       "2  fGwBva6hikQmTJzrbz/2Ezjm5Cth5jZUNvXigKK2AFA=         1\n",
       "3  mT5V8rEpa+8wuqi6x0DoVd3H5icMKkE9Prt49UlmK+4=         1\n",
       "4  XaPhtGLk/5UvvOYHcONTwsnH97P4eGECeq+BARGItRw=         1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data *(Iterative)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Why is the Data missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of tables\n",
    "data_tables = {'WRK_users_logs': user_logs.columns.tolist(),\n",
    "               'WRK_transactions_v1': transactions.columns.tolist(),\n",
    "               'WRK_members_v3': members.columns.tolist(),\n",
    "               'WRK_train_v1': churn.columns.tolist()\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Detect Nonsense Values in Dataset: via Datatype</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>column_name</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>msno</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>num_25</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>num_50</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>num_75</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>num_985</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>num_100</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>num_unq</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>total_secs</td>\n",
       "      <td>FLOAT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>WRK_users_logs</td>\n",
       "      <td>date</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>msno</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>payment_method_id</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>payment_plan_days</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>plan_list_price</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>actual_amount_paid</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>is_auto_renew</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>is_cancel</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>transaction_date</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>WRK_transactions_v1</td>\n",
       "      <td>membership_expire_date</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WRK_members_v3</td>\n",
       "      <td>msno</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>WRK_members_v3</td>\n",
       "      <td>city</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>WRK_members_v3</td>\n",
       "      <td>bd</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>WRK_members_v3</td>\n",
       "      <td>gender</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>WRK_members_v3</td>\n",
       "      <td>registered_via</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>WRK_members_v3</td>\n",
       "      <td>registration_init_time</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>WRK_train_v1</td>\n",
       "      <td>msno</td>\n",
       "      <td>STRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>WRK_train_v1</td>\n",
       "      <td>is_churn</td>\n",
       "      <td>INT64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            table_name             column_name data_type\n",
       "0       WRK_users_logs                    msno    STRING\n",
       "1       WRK_users_logs                  num_25     INT64\n",
       "2       WRK_users_logs                  num_50     INT64\n",
       "3       WRK_users_logs                  num_75     INT64\n",
       "4       WRK_users_logs                 num_985     INT64\n",
       "5       WRK_users_logs                 num_100     INT64\n",
       "6       WRK_users_logs                 num_unq     INT64\n",
       "7       WRK_users_logs              total_secs   FLOAT64\n",
       "8       WRK_users_logs                    date      DATE\n",
       "0  WRK_transactions_v1                    msno    STRING\n",
       "1  WRK_transactions_v1       payment_method_id     INT64\n",
       "2  WRK_transactions_v1       payment_plan_days     INT64\n",
       "3  WRK_transactions_v1         plan_list_price     INT64\n",
       "4  WRK_transactions_v1      actual_amount_paid     INT64\n",
       "5  WRK_transactions_v1           is_auto_renew     INT64\n",
       "6  WRK_transactions_v1               is_cancel     INT64\n",
       "7  WRK_transactions_v1        transaction_date      DATE\n",
       "8  WRK_transactions_v1  membership_expire_date      DATE\n",
       "0       WRK_members_v3                    msno    STRING\n",
       "1       WRK_members_v3                    city     INT64\n",
       "2       WRK_members_v3                      bd     INT64\n",
       "3       WRK_members_v3                  gender    STRING\n",
       "4       WRK_members_v3          registered_via     INT64\n",
       "5       WRK_members_v3  registration_init_time      DATE\n",
       "0         WRK_train_v1                    msno    STRING\n",
       "1         WRK_train_v1                is_churn     INT64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table with all table names, and their respective features and feature datatypes\n",
    "column_datatypes = pd.DataFrame()\n",
    "\n",
    "for x,y in data_tables.items():\n",
    "    QUERY = f\"\"\"\n",
    "                SELECT\n",
    "                 *\n",
    "                FROM\n",
    "                 `spark-kkbox.KKbox_User_Data`.INFORMATION_SCHEMA.COLUMNS\n",
    "                WHERE\n",
    "                 table_name='{x}'\n",
    "             \"\"\"\n",
    "    query = bigquery_client.query(QUERY)\n",
    "    df = query.to_dataframe()\n",
    "    column_datatypes = column_datatypes.append(df[['table_name','column_name','data_type']])\n",
    "    \n",
    "# Show Dataframe\n",
    "column_datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WRK_users_logs': [('msno', 'STRING'),\n",
       "  ('num_25', 'INT64'),\n",
       "  ('num_50', 'INT64'),\n",
       "  ('num_75', 'INT64'),\n",
       "  ('num_985', 'INT64'),\n",
       "  ('num_100', 'INT64'),\n",
       "  ('num_unq', 'INT64'),\n",
       "  ('total_secs', 'FLOAT64'),\n",
       "  ('date', 'DATE')],\n",
       " 'WRK_transactions_v1': [('msno', 'STRING'),\n",
       "  ('payment_method_id', 'INT64'),\n",
       "  ('payment_plan_days', 'INT64'),\n",
       "  ('plan_list_price', 'INT64'),\n",
       "  ('actual_amount_paid', 'INT64'),\n",
       "  ('is_auto_renew', 'INT64'),\n",
       "  ('is_cancel', 'INT64'),\n",
       "  ('transaction_date', 'DATE'),\n",
       "  ('membership_expire_date', 'DATE')],\n",
       " 'WRK_members_v3': [('msno', 'STRING'),\n",
       "  ('city', 'INT64'),\n",
       "  ('bd', 'INT64'),\n",
       "  ('gender', 'STRING'),\n",
       "  ('registered_via', 'INT64'),\n",
       "  ('registration_init_time', 'DATE')],\n",
       " 'WRK_train_v1': [('msno', 'STRING'), ('is_churn', 'INT64')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary with all features and fill with their respective column and datatype pairs\n",
    "column_data_pairs = {}\n",
    "for x,y in data_tables.items():\n",
    "    column_data_pairs[x] = list(zip(column_datatypes[column_datatypes['table_name'] == x]['column_name'], \n",
    "                                      column_datatypes[column_datatypes['table_name'] == x]['data_type']))\n",
    "# View dictionary\n",
    "column_data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the problematic values for the Table: WRK_users_logs\n",
      "- msno -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- num_25 -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- num_50 -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- num_75 -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- num_985 -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- num_100 -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- num_unq -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- total_secs -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- date -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "These are the problematic values for the Table: WRK_transactions_v1\n",
      "- msno -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- payment_method_id -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- payment_plan_days -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- plan_list_price -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- actual_amount_paid -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- is_auto_renew -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- is_cancel -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- transaction_date -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- membership_expire_date -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "These are the problematic values for the Table: WRK_members_v3\n",
      "- msno -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- city -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- bd -\n",
      "Empty DataFrame\n",
      "Columns: [msno, f0_]\n",
      "Index: []\n",
      "- gender -\n"
     ]
    }
   ],
   "source": [
    "# Print all values that cannot be converted to their respective feature's column type\n",
    "for x,y in column_data_pairs.items():\n",
    "    print(f\"These are the problematic values for the Table: {x}\")\n",
    "    for column,dtype in y:\n",
    "        print(f'- {column} -')\n",
    "        QUERY = f\"\"\"\n",
    "                    SELECT\n",
    "                      msno,\n",
    "                      SAFE_CAST({column} as {dtype})\n",
    "                    FROM `spark-kkbox.KKbox_User_Data.{x}`\n",
    "                    WHERE SAFE_CAST({column} as {dtype}) IS NULL\n",
    "                 \"\"\"\n",
    "        print(bigquery_client.query(QUERY).to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender was the only column that seem to have an issue. Let's take a deeper look into its unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "    gender,\n",
    "    count (*) as Total,\n",
    "    count(*) / (select count(*) from `spark-kkbox.KKbox_User_Data.WRK_members_v3`) as Relative_Perc\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_members_v3` \n",
    "GROUP BY gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None makes up 65% of the values for this feature. Such a large chunk like this missing does not allow us to properly imputate. We will be removing this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Detect Nonsense Values in Dataset: via Duplicate Entries</font> -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Query for Duplicate Entries in Transactions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "SELECT \n",
    "  COUNT(msno) AS num_members,\n",
    "  COUNT(DISTINCT msno) num_unq_members\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_transactions_v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Query for Duplicate Entries in Members***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "SELECT \n",
    "  COUNT(msno) AS num_members,\n",
    "  COUNT(DISTINCT msno) num_unq_members\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_members_v3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Query for Duplicate Entries in Memeber Churn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "SELECT \n",
    "  COUNT(msno) AS num_members,\n",
    "  COUNT(DISTINCT msno) num_unq_members\n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_train_v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both columns check out. However we now see that we have a much larger problem as our ***Member Churn data is significantly smaller than our Member data.*** This means that we do not have churn info for the majority of our Members:\n",
    "\n",
    "We will handle this by performing inner joins on most occasions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Detect Nonsense Values in Dataset: Less than Zero</font> -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Drop all rows where total_secs are Less Than ZERO***\n",
    "- There is a clear error in various observations in our User_Logs table with regards to Negative values for total_secs.\n",
    "- We will be dropping these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery \n",
    "DELETE  \n",
    "FROM `spark-kkbox.KKbox_User_Data.WRK_users_logs` \n",
    "WHERE total_secs < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Missing Data: Explore</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Empty DataFrame and populate it with missing values for each column of each table\n",
    "missing_values = pd.DataFrame()\n",
    "\n",
    "for x,y in data_tables.items():\n",
    "    for column in y:\n",
    "        QUERY = f\"\"\"\n",
    "        select \n",
    "          sum(case when d.{column} is null then 1 else 0 end) as Total_Missing, \n",
    "          sum(case when d.{column} is null then 1 else 0 end) / COUNT(*) as Percent_Missing\n",
    "        from `spark-kkbox.KKbox_User_Data.{x}` d;\n",
    "        \"\"\"\n",
    "        query2 = bigquery_client.query(QUERY)\n",
    "        df2 = query2.to_dataframe()\n",
    "        df2.index = [column]\n",
    "        missing_values = missing_values.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return missing_values table\n",
    "missing_values.sort_values('Total_Missing', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 65% data missing, we will be removing Gender form out analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Derived Tables and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Create our *Dervied (DRV)* Tables</font> -\n",
    "\n",
    "As previously stated, the Derived Tables are what we will build our model off of. These tables will be the result of a consolidation of all Working Tables into one single file for each month that is a part of our analysis. We will be using three months (January 2016, February 2016, and March 2016) as Training, Validation, and Testing sets respectively. To begin the construction of these tables let's define the business problem a little more in-depth.\n",
    "\n",
    "KKBOX defines churn as \"***no new valid service subscription within 30 days after the current membership expires***\". Although churn values are provided for members in the train_v1 dataset, they are the values for all members taken as of February 28, 2017. As we will be evaluating churn over several previous months we won't be able to use this churn data as it holds \"future\" information, thus we will need to calculate this ourselves. To do this first we will start by building our monthly Derived Tables with all members who have an <font color=green>*Membership Expiration Date*</font> that lay within each of the respected months. On top of this, we will also include all member-specific information from all tables as well as some simply derived features. Lastly we calculate <font color=green>*is_churn*</font> with respect to the definition. The result:\n",
    "- membership_expire_date\n",
    "- payment_method_id\n",
    "- payment_plan_days\n",
    "- plan_list_price\n",
    "- net_paid_amount\n",
    "- is_net_paid_amount\n",
    "- is_auto_renew\n",
    "- is_cancel\n",
    "- city\n",
    "- bd\n",
    "- registered_via\n",
    "- registration_init_time\n",
    "- membership_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of months and their data ranges\n",
    "months = {\n",
    "           'Jan2016': ['2016-01-01', '2016-01-31'],\n",
    "           'Feb2016': ['2016-02-01', '2016-02-28']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our initial Derived (DRV) Tables for each month\n",
    "for month, mrange in months.items():\n",
    "    QUERY = f\"\"\"\n",
    "                CREATE TABLE `spark-kkbox.KKbox_User_Data.DRV_{month}` AS\n",
    "                SELECT \n",
    "                    t.msno,\n",
    "                    t.membership_expire_date,\n",
    "                    t.payment_method_id,\n",
    "                    t.payment_plan_days,\n",
    "                    t.plan_list_price,\n",
    "                    t.plan_list_price - t.actual_amount_paid AS net_paid_amount,\n",
    "                    CASE WHEN t.plan_list_price - t.actual_amount_paid < 0 THEN 'over'\n",
    "                         WHEN t.plan_list_price - t.actual_amount_paid > 0 THEN 'under'\n",
    "                         ELSE 'neither' END AS is_net_paid_amount,\n",
    "                    t.is_auto_renew,\n",
    "                    t.is_cancel,\n",
    "                    m.city,\n",
    "                    m.bd,\n",
    "                    m.registered_via,\n",
    "                    m.registration_init_time, \n",
    "                    DATE_DIFF(membership_expire_date, registration_init_time, DAY) AS membership_length\n",
    "                FROM `spark-kkbox.KKbox_User_Data.WRK_transactions_v1` t\n",
    "                INNER JOIN `spark-kkbox.KKbox_User_Data.WRK_members_v3` m\n",
    "                ON m.msno = t.msno\n",
    "                WHERE t.membership_expire_date BETWEEN DATE('{mrange[0]}') and DATE('{mrange[1]}')\n",
    "                AND DATE_DIFF(membership_expire_date, registration_init_time, DAY) >= 30                 -- Filter for Aleast 30 days of usage\n",
    "             \"\"\"\n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This table will serve as our base for where we will add our new features onto. As all of our source tables capture different types of user behaviors over different timeframes we can't just join them all together immediately. However, as they do capture behavior over time I believe that there would be a great value if we can get creative on how we capture this ***retrospective data***. We will create these new features through additional queries and python commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete any duplicate rows from our *Dervied (DRV)* Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any duplicate rows from our DRV Tables\n",
    "for month, mrange in months.items():\n",
    "    QUERY = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `spark-kkbox.KKbox_User_Data.DRV_{month}` AS\n",
    "                SELECT * EXCEPT(rn)\n",
    "                FROM (\n",
    "                  SELECT *, ROW_NUMBER() OVER(PARTITION BY msno ORDER BY msno) rn\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.DRV_{month}`\n",
    "                ) \n",
    "                WHERE rn = 1 \n",
    "             \"\"\"\n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Iterative Feature Creation</font> -\n",
    "All Feature creation queries will be performed here. First we will create all feature columns directly in the schemas of our DRV Tables through API requests\n",
    "\n",
    "It is important to keep in mind that our features will be created with respect to the timeframe in which they are being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature columns to all DRV Tables via schema API\n",
    "\n",
    "for month in months.keys():\n",
    "    table_ref = bigquery_client.dataset('KKbox_User_Data').table(f'DRV_{month}')\n",
    "    table = bigquery_client.get_table(table_ref)  # API request\n",
    "    original_schema = table.schema\n",
    "    new_schema = original_schema[:]  # creates a copy of the schema\n",
    "    \n",
    "    # Aggregate Features\n",
    "    new_schema.append(bigquery.SchemaField(\"is_churn\", \"FLOAT64\")) \n",
    "    new_schema.append(bigquery.SchemaField(\"total_songs\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"total_logins\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"total_secs\", \"FLOAT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"sum_num_unq\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"sum_num_repeat\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"sum_over_50pec\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"sum_over_75pec\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"sum_over_985pec\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"total_transactions\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"total_spent\", \"FLOAT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"avg_spent_trans\", \"FLOAT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"spent_per_logins\", \"FLOAT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"spent_per_secs\", \"FLOAT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"spent_per_song\", \"FLOAT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"spent_per_num_unq\", \"FLOAT64\")) \n",
    "    new_schema.append(bigquery.SchemaField(\"spent_per_num_repeats\", \"FLOAT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"never_active_subscriber\", \"FLOAT64\")) \n",
    "    new_schema.append(bigquery.SchemaField(\"total_spent_zero\", \"FLOAT64\")) \n",
    "    new_schema.append(bigquery.SchemaField(\"city_agg\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"payment_method_agg\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"expire_last_login\", \"INT64\"))\n",
    "    new_schema.append(bigquery.SchemaField(\"total_cancelations\", \"INT64\"))\n",
    "    \n",
    "    table.schema = new_schema\n",
    "    table = bigquery_client.update_table(table, [\"schema\"])  # API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature columns to all DRV Tables via schema API\n",
    "\n",
    "for month in months.keys():\n",
    "    table_ref = bigquery_client.dataset('KKbox_User_Data').table(f'DRV_{month}')\n",
    "    table = bigquery_client.get_table(table_ref)  # API request\n",
    "    original_schema = table.schema\n",
    "    new_schema = original_schema[:]  # creates a copy of the schema\n",
    "    \n",
    "    # Retrospective Aggregates\n",
    "    intervals = [7,15,30,60,120]\n",
    "    for interval in intervals:\n",
    "        new_schema.append(bigquery.SchemaField(f\"songs_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"songs_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"logins_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"logins_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"total_secs_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"total_secs_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"num_unq_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"num_unq_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"num_repeat_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"num_repeat_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"over_50perc_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"over_50perc_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"over_75perc_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"over_75perc_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"over_985perc_last_{interval}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"over_985perc_last_{interval}_AVG\", \"FLOAT64\"))\n",
    "\n",
    "        table.schema = new_schema\n",
    "        table = bigquery_client.update_table(table, [\"schema\"])  # API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature columns to all DRV Tables via schema API\n",
    "\n",
    "for month in months.keys():\n",
    "    table_ref = bigquery_client.dataset('KKbox_User_Data').table(f'DRV_{month}')\n",
    "    table = bigquery_client.get_table(table_ref)  # API request\n",
    "    original_schema = table.schema\n",
    "    new_schema = original_schema[:]  # creates a copy of the schema\n",
    "\n",
    "    for interval in [10,20,30]:\n",
    "        new_schema.append(bigquery.SchemaField(f\"login_after_expire_{interval}\", \"INT64\"))\n",
    "\n",
    "        table.schema = new_schema\n",
    "        table = bigquery_client.update_table(table, [\"schema\"])  # API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature columns to all DRV Tables via schema API\n",
    "\n",
    "for month in months.keys():\n",
    "    table_ref = bigquery_client.dataset('KKbox_User_Data').table(f'DRV_{month}')\n",
    "    table = bigquery_client.get_table(table_ref)  # API request\n",
    "    original_schema = table.schema\n",
    "    new_schema = original_schema[:]  # creates a copy of the schema\n",
    "\n",
    "    blocks = [0,15,30,45,60]\n",
    "    for i in range(0,4):\n",
    "        new_schema.append(bigquery.SchemaField(f\"SUM_unq_songs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"AVG_unq_songs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"STD_unq_songs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"SUM_songs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"AVG_songs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"STD_songs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"SUM_secs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"AVG_secs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"STD_secs_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"SUM_songs50_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"AVG_songs50_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"STD_songs50_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"SUM_logins_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"AVG_logins_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"SUM_repeats_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"AVG_repeats_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"STD_repeats_{blocks[i]}_{blocks[i+1]}\", \"FLOAT64\"))\n",
    "\n",
    "        table.schema = new_schema\n",
    "        table = bigquery_client.update_table(table, [\"schema\"])  # API request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature columns to all DRV Tables via schema API\n",
    "\n",
    "for month in months.keys():\n",
    "    table_ref = bigquery_client.dataset('KKbox_User_Data').table(f'DRV_{month}')\n",
    "    table = bigquery_client.get_table(table_ref)  # API request\n",
    "    original_schema = table.schema\n",
    "    new_schema = original_schema[:]  # creates a copy of the schema\n",
    "\n",
    "    blocks = [0,15,30,45,60]\n",
    "    for i in range(0,3):\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSUM_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFAVG_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSTD_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        \n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSUM_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFAVG_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSTD_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSUM_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFAVG_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSTD_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSUM_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFAVG_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSTD_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        \n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSUM_logins_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFAVG_logins_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        \n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSUM_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFAVG_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))  \n",
    "        new_schema.append(bigquery.SchemaField(f\"DIFSTD_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]}\", \"FLOAT64\"))\n",
    "\n",
    "        table.schema = new_schema\n",
    "        table = bigquery_client.update_table(table, [\"schema\"])  # API request   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple> #1 Aggregate *(Activity)* Table Features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**current_is_auto_renew**: how much does the member pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added via DRV Table creation query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**current_method_id**: What is the member's current payment plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added via DRV Table creation query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**current_payment_plan**: What is the member's current payment plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added via DRV Table creation query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**current_plan_list_price**: how much does the member pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added via DRV Table creation query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**net_paid_amount**: How much member over/under pay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added via DRV Table creation query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**is_net_paid_amount**: Did member over/under pay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added via DRV Table creation query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**is_cancel**: Did member cancel in most recent transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added via DRV Table creation query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>#2 Aggregate *User Log (Activity)* Table Features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 2.1*** - General Aggregates\n",
    "- **total_songs**: Total songs played over lifetime\n",
    "- **total_longs**: Total songs played over lifetime\n",
    "- **total_secs**: Total songs played over lifetime\n",
    "- **sum_num_unq**: Total songs played over lifetime\n",
    "- **sum_num_repeat**: Total songs played over lifetime\n",
    "- **sum_over_50perc**: Total songs played over lifetime\n",
    "- **sum_over_75perc**: Total songs played over lifetime\n",
    "- **sum_over_985perc**: Total songs played over lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Values to Zero to avoid NULLS\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_songs = 0,\n",
    "                    total_logins = 0,\n",
    "                    total_secs = 0,\n",
    "                    sum_num_unq = 0,\n",
    "                    sum_num_repeat = 0,\n",
    "                    sum_over_50pec = 0,\n",
    "                    sum_over_75pec = 0,\n",
    "                    sum_over_985pec = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate features with data\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_songs = f.total_songs1,\n",
    "                    total_logins = f.total_logins1,\n",
    "                    total_secs = f.total_secs1,\n",
    "                    sum_num_unq = f.sum_num_unq1,\n",
    "                    sum_num_repeat = f.sum_num_repeat1,\n",
    "                    sum_over_50pec = f.sum_over_50pec1,\n",
    "                    sum_over_75pec = f.sum_over_75pec1,\n",
    "                    sum_over_985pec = f.sum_over_985pec1\n",
    "                FROM (\n",
    "                  SELECT * \n",
    "                  FROM `spark-kkbox.KKbox_User_Data.DRV_{month}` d \n",
    "                  INNER JOIN( \n",
    "                      SELECT \n",
    "                        x.msno as member,\n",
    "                        sum(num_25 + num_50 + num_75 + num_985 + num_100) AS total_songs1,\n",
    "                        count(x.msno) AS total_logins1,\n",
    "                        sum(x.total_secs) AS total_secs1,\n",
    "                        sum(num_unq) AS sum_num_unq1,\n",
    "                        sum(num_25 + num_50 + num_75 + num_985 + num_100) - sum(num_unq) AS sum_num_repeat1,\n",
    "                        sum(num_50 + num_75 + num_985 + num_100) AS sum_over_50pec1,\n",
    "                        sum(num_75 + num_985 + num_100) AS sum_over_75pec1,\n",
    "                        sum(num_985 + num_100) AS sum_over_985pec1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.WRK_users_logs`x\n",
    "                  JOIN `spark-kkbox.KKbox_User_Data.DRV_{month}` u\n",
    "                  ON x.msno = u.msno\n",
    "                  WHERE x.date <= u.membership_expire_date \n",
    "                  GROUP BY x.msno) as sub_q \n",
    "                ON d.msno = sub_q.member ) as f \n",
    "                WHERE a.msno = f.msno\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unfortunately BigQuery does not allow for Aggregations on the UPDATE Clause. In order to go around this inconvenience I was forced to perform the above subquery*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>3) Retrospective *User Log (Activity)* Table Features</font> -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 3.1***  - Aggregate Data over Various Time Intervals\n",
    "- **songs_last_{interval}**: How many songs listened to within 7 days of membership_expire_date\n",
    "- **songs_last_{interval}_AVG**: SMA many songs listened to within 7 days of membership_expire_date\n",
    "- **logins_last_{interval}**: How many logins within 7 days of membership_expire_date\n",
    "- **logins_last_{interval}_AVG**: How many logins within 7 days of membership_expire_date\n",
    "- **total_secs_last_{interval}**: How many seconds of music listened to within 7 days of membership_expire_date\n",
    "- **total_secs_last_{interval}_AVG**: How many seconds of music listened to within 7 days of membership_expire_date\n",
    "- **num_unq_last_{interval}**: How many unique songs listened to within 7 days of membership_expire_date\n",
    "- **num_unq_last_{interval}_AVG**: How many unique songs listened to within 7 days of membership_expire_date\n",
    "- **num_repeat_last_{interval}**: How many repeat songs listened to within 7 days of membership_expire_date\n",
    "- **num_repeat_last_{interval}_AVG**: How many repeat songs listened to within 7 days of membership_expire_date\n",
    "- **over_50perc_last_{interval}**: How many songs were listend to over 50% within 7 days of membership_expire_date\n",
    "- **over_50perc_last_{interval}_AVG**: How many songs were listend to over 50% within 7 days of membership_expire_date\n",
    "- **over_75perc_last_{interval}**: How many songs were listend to over 75% within 7 days of membership_expire_date\n",
    "- **over_75perc_last_{interval}_AVG**: How many songs were listend to over 75% within 7 days of membership_expire_date\n",
    "- **over_985perc_last_{interval}**: How many songs were listend to over 98.5% within 7 days of membership_expire_date\n",
    "- **over_985perc_last_{interval}_AVG**: How many songs were listend to over 98.5% within 7 days of membership_expire_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of day intervals for retrospective features\n",
    "intervals = [7,15,30,60,120]\n",
    "\n",
    "for month in months.keys():\n",
    "    for interval in intervals:\n",
    "        QUERY = f\"\"\"\n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET songs_last_{interval} = 0,\n",
    "                        songs_last_{interval}_AVG = 0,\n",
    "                        logins_last_{interval} = 0,\n",
    "                        logins_last_{interval}_AVG = 0,\n",
    "                        total_secs_last_{interval} = 0,\n",
    "                        total_secs_last_{interval}_AVG = 0,\n",
    "                        num_unq_last_{interval} = 0,\n",
    "                        num_unq_last_{interval}_AVG = 0,\n",
    "                        num_repeat_last_{interval} = 0,\n",
    "                        num_repeat_last_{interval}_AVG = 0,\n",
    "                        over_50perc_last_{interval} = 0,\n",
    "                        over_50perc_last_{interval}_AVG = 0,\n",
    "                        over_75perc_last_{interval} = 0,\n",
    "                        over_75perc_last_{interval}_AVG = 0,\n",
    "                        over_985perc_last_{interval} = 0,\n",
    "                        over_985perc_last_{interval}_AVG = 0\n",
    "                    WHERE TRUE\n",
    "                 \"\"\"\n",
    "\n",
    "        # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of day intervals for retrospective features\n",
    "intervals = [7,15,30,60,120]\n",
    "\n",
    "for month in months.keys():\n",
    "    for interval in intervals:\n",
    "        QUERY = f\"\"\"\n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET songs_last_{interval} = f.songs_last_{interval}1,\n",
    "                        songs_last_{interval}_AVG = f.songs_last_{interval}_AVG1,\n",
    "                        logins_last_{interval} = f.logins_last_{interval}1,\n",
    "                        logins_last_{interval}_AVG = f.logins_last_{interval}_AVG1,\n",
    "                        total_secs_last_{interval} = f.total_secs_last_{interval}1,\n",
    "                        total_secs_last_{interval}_AVG = f.total_secs_last_{interval}_AVG1,\n",
    "                        num_unq_last_{interval} = f.num_unq_last_{interval}1,\n",
    "                        num_unq_last_{interval}_AVG = f.num_unq_last_{interval}_AVG1,\n",
    "                        num_repeat_last_{interval} = f.num_repeat_last_{interval}1,\n",
    "                        num_repeat_last_{interval}_AVG = f.num_repeat_last_{interval}_AVG1,\n",
    "                        over_50perc_last_{interval} = f.over_50perc_last_{interval}1,\n",
    "                        over_50perc_last_{interval}_AVG = f.over_50perc_last_{interval}_AVG1,\n",
    "                        over_75perc_last_{interval} = f.over_75perc_last_{interval}1,\n",
    "                        over_75perc_last_{interval}_AVG = f.over_75perc_last_{interval}_AVG1,\n",
    "                        over_985perc_last_{interval} = f.over_985perc_last_{interval}1,\n",
    "                        over_985perc_last_{interval}_AVG = f.over_985perc_last_{interval}_AVG1\n",
    "                    FROM (\n",
    "                      SELECT * \n",
    "                      FROM `spark-kkbox.KKbox_User_Data.DRV_{month}` d \n",
    "                      INNER JOIN( \n",
    "                          SELECT \n",
    "                            u.msno as member,\n",
    "                            sum(num_25 + num_50 + num_75 + num_985 + num_100) AS songs_last_{interval}1,\n",
    "                            sum(num_25 + num_50 + num_75 + num_985 + num_100)/{interval} AS songs_last_{interval}_AVG1,\n",
    "                            count(u.msno) AS logins_last_{interval}1,\n",
    "                            count(u.msno)/{interval} AS logins_last_{interval}_AVG1,\n",
    "                            sum(u.total_secs) AS total_secs_last_{interval}1,\n",
    "                            sum(u.total_secs)/{interval}  AS total_secs_last_{interval}_AVG1,\n",
    "                            sum(num_unq) AS num_unq_last_{interval}1,\n",
    "                            sum(num_unq)/{interval}  AS num_unq_last_{interval}_AVG1,\n",
    "                            sum(num_25 + num_50 + num_75 + num_985 + num_100) - sum(num_unq) AS num_repeat_last_{interval}1,\n",
    "                            (sum(num_25 + num_50 + num_75 + num_985 + num_100) - sum(num_unq))/{interval}  AS num_repeat_last_{interval}_AVG1,\n",
    "                            sum(num_50 + num_75 + num_985 + num_100) AS over_50perc_last_{interval}1,\n",
    "                            sum(num_50 + num_75 + num_985 + num_100)/{interval} AS over_50perc_last_{interval}_AVG1,\n",
    "                            sum(num_75 + num_985 + num_100) AS over_75perc_last_{interval}1,\n",
    "                            sum(num_75 + num_985 + num_100)/{interval} AS over_75perc_last_{interval}_AVG1,\n",
    "                            sum(num_985 + num_100) AS over_985perc_last_{interval}1,\n",
    "                            sum(num_985 + num_100)/{interval} AS over_985perc_last_{interval}_AVG1\n",
    "                        FROM `spark-kkbox.KKbox_User_Data.WRK_users_logs` u\n",
    "                        JOIN `spark-kkbox.KKbox_User_Data.DRV_{month}` d\n",
    "                        ON u.msno = d.msno\n",
    "                        WHERE u.date BETWEEN DATE_SUB(d.membership_expire_date, INTERVAL {interval} DAY) AND d.membership_expire_date\n",
    "                        GROUP BY member) as sub_q\n",
    "                    ON d.msno = sub_q.member ) as f \n",
    "                    WHERE a.msno = f.msno\n",
    "                 \"\"\"\n",
    "\n",
    "        # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 3.2***  - Recent Login Activity\n",
    "- **expire_last_login**: Number of days since last login before expiration date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                SET expire_last_login = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "\n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                SET expire_last_login = expire_last_login1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                    u.msno as member,\n",
    "                    DATE_DIFF(max(membership_expire_date), max(x.date), DAY) as expire_last_login1          \n",
    "                  FROM `spark-kkbox.KKbox_User_Data.WRK_users_logs`x\n",
    "                  JOIN `spark-kkbox.KKbox_User_Data.DRV_{month}` u\n",
    "                  ON x.msno = u.msno\n",
    "                  WHERE x.date <= u.membership_expire_date \n",
    "                  GROUP BY u.msno\n",
    "                  ) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "\n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **login_after_expire_DAYS**: Did user login after expiration date within 10, 20, 30 days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [9,19,29]\n",
    "for month in months.keys():\n",
    "    for interval in intervals:\n",
    "        QUERY = f\"\"\"\n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET login_after_expire_{interval} = 0\n",
    "                    WHERE TRUE\n",
    "                 \"\"\"\n",
    "\n",
    "        # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [10,20,30]\n",
    "for month in months.keys():\n",
    "    for interval in intervals:\n",
    "        QUERY = f\"\"\"                \n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET login_after_expire_{interval}  = login_after_expire_{interval}1\n",
    "                    FROM (\n",
    "                      SELECT \n",
    "                        u.msno as member,\n",
    "                        CASE WHEN max(x.date) BETWEEN max(u.membership_expire_date) AND DATE_ADD(max(u.membership_expire_date), INTERVAL {interval}  DAY) THEN 1\n",
    "                        ELSE 0 END AS login_after_expire_{interval}1\n",
    "                      FROM `spark-kkbox.KKbox_User_Data.WRK_users_logs`x\n",
    "                      JOIN `spark-kkbox.KKbox_User_Data.DRV_{month}` u\n",
    "                      ON x.msno = u.msno\n",
    "                      WHERE x.date BETWEEN u.membership_expire_date AND DATE_ADD(u.membership_expire_date, INTERVAL {interval-1} DAY)\n",
    "                      GROUP BY u.msno\n",
    "                      ) as sub_q \n",
    "                      WHERE a.msno = sub_q.member\n",
    "                 \"\"\"\n",
    "\n",
    "        # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 3.4***  - Bi-Weekly Activity Blocks\n",
    "- **SUM_unq_songs_{start}_{end}**: SUM Unique Songs within interval\n",
    "- **AVG_unq_songs_{start}_{end}**: AVG Unique Songs within interval\n",
    "- **STD_unq_songs_{start}_{end}**: STD Unique Songs within interval\n",
    "\n",
    "- **SUM_songs_{start}_{end}**: SUM Songs within interval\n",
    "- **AVG_songs_{start}_{end}**: AVG Songs within interval\n",
    "- **STD_songs_{start}_{end}**: STD Songs within interval\n",
    "\n",
    "- **SUM_secs_{start}_{end}**: SUM Seconds within interval\n",
    "- **AVG_secs_{start}_{end}**: AVG Seconds within interval\n",
    "- **STD_secs_{start}_{end}**: STD Seconds within interval\n",
    "\n",
    "- **SUM_songs50_{start}_{end}**: SUM Songs Played Under 50% within interval\n",
    "- **AVG_songs50_{start}_{end}**: AVG Songs Played Under 50% within interval\n",
    "- **STD_songs50_{start}_{end}**: STD Songs Played Under 50% within interval\n",
    "\n",
    "- **SUM_logins_{start}_{end}**: SUM Logins within interval\n",
    "- **AVG_logins_{start}_{end}**: AVG Logins within interval\\\n",
    "\n",
    "- **SUM_repeats_{start}_{end}**: SUM Repeat Songs Played within interval\n",
    "- **AVG_repeats_{start}_{end}**: AVG Repeat Songs Played within interval\n",
    "- **STD_repeats_{start}_{end}**: STD Repeat Songs Played within interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [0,15,30,45,60]\n",
    "for month in months.keys():\n",
    "    for i in range(0,4):\n",
    "        QUERY = f\"\"\"\n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET \n",
    "                        SUM_unq_songs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        AVG_unq_songs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        STD_unq_songs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "\n",
    "                        SUM_songs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        AVG_songs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        STD_songs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "\n",
    "                        SUM_secs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        AVG_secs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        STD_secs_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        \n",
    "                        SUM_songs50_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        AVG_songs50_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        STD_songs50_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        \n",
    "                        SUM_logins_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        AVG_logins_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "\n",
    "                        SUM_repeats_{blocks[i]}_{blocks[i+1]} = 0,\n",
    "                        AVG_repeats_{blocks[i]}_{blocks[i+1]} = 0,                     \n",
    "                        STD_repeats_{blocks[i]}_{blocks[i+1]} = 0\n",
    "                    WHERE TRUE\n",
    "                 \"\"\"\n",
    "\n",
    "        # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [0,15,30,45,60]\n",
    "for month in months.keys():\n",
    "    for i in range(0,4):\n",
    "        QUERY = f\"\"\"\n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET \n",
    "                        SUM_unq_songs_{blocks[i]}_{blocks[i+1]} = SUM_unq_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        AVG_unq_songs_{blocks[i]}_{blocks[i+1]} = AVG_unq_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        STD_unq_songs_{blocks[i]}_{blocks[i+1]} = STD_unq_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "\n",
    "                        SUM_songs_{blocks[i]}_{blocks[i+1]} = SUM_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        AVG_songs_{blocks[i]}_{blocks[i+1]} = AVG_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        STD_songs_{blocks[i]}_{blocks[i+1]} = STD_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "\n",
    "                        SUM_secs_{blocks[i]}_{blocks[i+1]} = SUM_secs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        AVG_secs_{blocks[i]}_{blocks[i+1]} = AVG_secs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        STD_secs_{blocks[i]}_{blocks[i+1]} = STD_secs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        \n",
    "                        SUM_songs50_{blocks[i]}_{blocks[i+1]} = SUM_songs50_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        AVG_songs50_{blocks[i]}_{blocks[i+1]} = AVG_songs50_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        STD_songs50_{blocks[i]}_{blocks[i+1]} = STD_songs50_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        \n",
    "                        SUM_logins_{blocks[i]}_{blocks[i+1]} = SUM_logins_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        AVG_logins_{blocks[i]}_{blocks[i+1]} = AVG_logins_{blocks[i]}_{blocks[i+1]}1,\n",
    "\n",
    "                        SUM_repeats_{blocks[i]}_{blocks[i+1]} = SUM_repeats_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        AVG_repeats_{blocks[i]}_{blocks[i+1]} = AVG_repeats_{blocks[i]}_{blocks[i+1]}1,                     \n",
    "                        STD_repeats_{blocks[i]}_{blocks[i+1]} = STD_repeats_{blocks[i]}_{blocks[i+1]}1\n",
    "                                                \n",
    "                    FROM (\n",
    "                      SELECT \n",
    "                        u.msno as member,\n",
    "\n",
    "                        SUM(num_25 + num_50 + num_75 + num_985 + num_100) as SUM_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        SUM(num_25 + num_50 + num_75 + num_985 + num_100)/15 as AVG_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        CASE WHEN COUNT(u.msno) < 3 THEN 0\n",
    "                        ELSE STDDEV(num_25 + num_50 + num_75 + num_985 + num_100) END AS STD_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "\n",
    "                        SUM(u.total_secs) as SUM_secs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        SUM(u.total_secs)/15 as AVG_secs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        CASE WHEN COUNT(u.msno) < 3 THEN 0\n",
    "                        ELSE STDDEV(u.total_secs) END AS STD_secs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        \n",
    "                        SUM(num_unq) as SUM_unq_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        SUM(num_unq)/15 as AVG_unq_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        CASE WHEN COUNT(u.msno) < 3 THEN 0\n",
    "                        ELSE STDDEV(num_unq) END AS STD_unq_songs_{blocks[i]}_{blocks[i+1]}1,\n",
    "                       \n",
    "                        SUM(num_25 + num_50 + num_75 + num_985 + num_100 - num_unq) as SUM_repeats_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        SUM(num_25 + num_50 + num_75 + num_985 + num_100 - num_unq)/15 as AVG_repeats_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        CASE WHEN COUNT(u.msno) < 3 THEN 0\n",
    "                        ELSE STDDEV(num_25 + num_50 + num_75 + num_985 + num_100 - num_unq) END AS STD_repeats_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        \n",
    "                        COUNT(u.msno) as SUM_logins_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        COUNT(u.msno)/15 as AVG_logins_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        \n",
    "                        SUM(num_25 + num_50) as SUM_songs50_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        SUM(num_25 + num_50)/15 as AVG_songs50_{blocks[i]}_{blocks[i+1]}1,\n",
    "                        CASE WHEN COUNT(u.msno) < 3 THEN 0\n",
    "                        ELSE STDDEV(num_25 + num_50) END AS STD_songs50_{blocks[i]}_{blocks[i+1]}1\n",
    "\n",
    "                        FROM `spark-kkbox.KKbox_User_Data.WRK_users_logs` u\n",
    "                        JOIN `spark-kkbox.KKbox_User_Data.DRV_{month}` d\n",
    "                        ON u.msno = d.msno\n",
    "                        WHERE u.date BETWEEN DATE_SUB(d.membership_expire_date, INTERVAL {blocks[i+1]} DAY) \n",
    "                                     AND DATE_SUB(d.membership_expire_date, INTERVAL {blocks[i]} DAY)\n",
    "                        GROUP BY member) as sub_q\n",
    "                    WHERE a.msno = sub_q.member\n",
    "                 \"\"\"\n",
    "\n",
    "        # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 3.4***  - Comparative, Prior Bi-Weekly Activity Blocks\n",
    "\n",
    "- **DIFSUM_unq_songs_{start}_{end}**: SUM Unique Songs within interval\n",
    "- **DIFAVG_unq_songs_{start}_{end}**: AVG Unique Songs within interval\n",
    "- **DIFSTD_unq_songs_{start}_{end}**: STD Unique Songs within interval\n",
    "\n",
    "- **DIFSUM_songs_{start}_{end}**: SUM Songs within interval\n",
    "- **DIFAVG_songs_{start}_{end}**: AVG Songs within interval\n",
    "- **DIFSTD_songs_{start}_{end}**: STD Songs within interval\n",
    "\n",
    "- **DIFSUM_secs_{start}_{end}**: SUM Seconds within interval\n",
    "- **DIFAVG_secs_{start}_{end}**: AVG Seconds within interval\n",
    "- **DIFSTD_secs_{start}_{end}**: STD Seconds within interval\n",
    "\n",
    "- **DIFSUM_songs50_{start}_{end}**: SUM Songs Played Under 50% within interval\n",
    "- **DIFAVG_songs50_{start}_{end}**: AVG Songs Played Under 50% within interval\n",
    "- **DIFSTD_songs50_{start}_{end}**: STD Songs Played Under 50% within interval\n",
    "\n",
    "- **DIFSUM_logins_{start}_{end}**: SUM Logins within interval\n",
    "- **DIFAVG_logins_{start}_{end}**: AVG Logins within interval\n",
    "- **DIFSTD_logins_{start}_{end}**: STD Logins within interval\n",
    "\n",
    "- **DIFSUM_repeats_{start}_{end}**: SUM Repeat Songs Played within interval\n",
    "- **DIFAVG_repeats_{start}_{end}**: AVG Repeat Songs Played within interval\n",
    "- **DIFSTD_repeats_{start}_{end}**: STD Repeat Songs Played within interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [0,15,30,45,60]\n",
    "for month in months.keys():\n",
    "    for i in range(0,3):\n",
    "        QUERY = f\"\"\"\n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET \n",
    "                        DIFSUM_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFAVG_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFSTD_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "\n",
    "                        DIFSUM_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFAVG_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFSTD_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "\n",
    "                        DIFSUM_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFAVG_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFSTD_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "\n",
    "                        DIFSUM_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFAVG_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFSTD_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        \n",
    "                        DIFSUM_logins_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFAVG_logins_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "\n",
    "                        DIFSUM_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFAVG_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0,\n",
    "                        DIFSTD_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = 0\n",
    "                    WHERE TRUE\n",
    "                 \"\"\"\n",
    "\n",
    "#         Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [0,15,30,45,60]\n",
    "for month in months.keys():\n",
    "    for i in range(0,3):\n",
    "        QUERY = f\"\"\"\n",
    "                    UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a\n",
    "                    SET \n",
    "                        DIFSUM_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = SUM_unq_songs_{blocks[i+1]}_{blocks[i+2]} - SUM_unq_songs_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFAVG_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = AVG_unq_songs_{blocks[i+1]}_{blocks[i+2]} - AVG_unq_songs_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFSTD_unq_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = STD_unq_songs_{blocks[i+1]}_{blocks[i+2]} - STD_unq_songs_{blocks[i]}_{blocks[i+1]},\n",
    "                        \n",
    "                        DIFSUM_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = SUM_songs_{blocks[i+1]}_{blocks[i+2]} - SUM_songs_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFAVG_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = AVG_songs_{blocks[i+1]}_{blocks[i+2]} - AVG_songs_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFSTD_songs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = STD_songs_{blocks[i+1]}_{blocks[i+2]} - STD_songs_{blocks[i]}_{blocks[i+1]},\n",
    "                        \n",
    "                        DIFSUM_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = SUM_secs_{blocks[i+1]}_{blocks[i+2]} - SUM_secs_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFAVG_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = AVG_secs_{blocks[i+1]}_{blocks[i+2]} - AVG_secs_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFSTD_secs_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = STD_secs_{blocks[i+1]}_{blocks[i+2]} - STD_secs_{blocks[i]}_{blocks[i+1]},\n",
    "                \n",
    "                        DIFSUM_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = SUM_songs50_{blocks[i+1]}_{blocks[i+2]} - SUM_songs50_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFAVG_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = AVG_songs50_{blocks[i+1]}_{blocks[i+2]} - AVG_songs50_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFSTD_songs50_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = STD_songs50_{blocks[i+1]}_{blocks[i+2]} - STD_songs50_{blocks[i]}_{blocks[i+1]},\n",
    "                        \n",
    "                        DIFSUM_logins_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = SUM_logins_{blocks[i+1]}_{blocks[i+2]} - SUM_logins_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFAVG_logins_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = AVG_logins_{blocks[i+1]}_{blocks[i+2]} - AVG_logins_{blocks[i]}_{blocks[i+1]},\n",
    "\n",
    "                        DIFSUM_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = SUM_repeats_{blocks[i+1]}_{blocks[i+2]} - SUM_repeats_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFAVG_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = AVG_repeats_{blocks[i+1]}_{blocks[i+2]} - AVG_repeats_{blocks[i]}_{blocks[i+1]},\n",
    "                        DIFSTD_repeats_{blocks[i]}_{blocks[i+1]}_{blocks[i+1]}_{blocks[i+2]} = STD_repeats_{blocks[i+1]}_{blocks[i+2]} - STD_repeats_{blocks[i]}_{blocks[i+1]}\n",
    "                        \n",
    "                    WHERE True\n",
    "                 \"\"\"\n",
    "\n",
    "        # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "        bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>4) Aggregate *Transactions* Table Features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 4.1*** - General Aggregates\n",
    "\n",
    "- **total_transactions**: How many transactions in a member's lifetime\n",
    "- **total_spent**: Total amount of money spent\n",
    "- **avg_spent_trans**: How much money per transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Values to Zero to avoid NULLS\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_transactions = 0,\n",
    "                    total_spent = 0,\n",
    "                    avg_spent_trans = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate features with data\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_transactions = total_transactions1,\n",
    "                    total_spent = total_spent1,\n",
    "                    avg_spent_trans = avg_spent_trans1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                        t.msno as member,\n",
    "                        count(t.msno) as total_transactions1,\n",
    "                        sum(actual_amount_paid) as total_spent1,\n",
    "                        sum(actual_amount_paid) / count(t.msno) as avg_spent_trans1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.WRK_transactions_v1` t\n",
    "                  JOIN `spark-kkbox.KKbox_User_Data.DRV_{month}` w\n",
    "                  ON t.msno = w.msno\n",
    "                  WHERE t.transaction_date < w.membership_expire_date\n",
    "                  GROUP BY member) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 4.2***\n",
    "\n",
    "- **total_spent_zero**: Binary for Member's who have spent a total of zero. This is mainly to avoid '0 divide by 0' errors\n",
    "- **never_active_subscriber**: Binary for Member's who pay each month but have never used the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Values to Zero to avoid NULLS\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_spent_zero = 0,\n",
    "                    never_active_subscriber = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate features with data\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_spent_zero = total_spent_zero1,\n",
    "                    never_active_subscriber = never_active_subscriber1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                    msno as member,\n",
    "                    CASE WHEN total_spent = 0 THEN 1\n",
    "                    ELSE 0 END AS total_spent_zero1,\n",
    "                    CASE WHEN total_spent > 0 AND total_spent_zero = 0 AND (total_secs = 0 or total_songs = 0 or total_logins = 0 or sum_num_unq = 0 or sum_num_repeat = 0) THEN 1\n",
    "                    ELSE 0 END AS never_active_subscriber1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.DRV_{month}`) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>never_used_memberships</th>\n",
       "      <th>total_amount_spent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19203</td>\n",
       "      <td>22224274.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   never_used_memberships  total_amount_spent\n",
       "0                   19203          22224274.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT \n",
    "    count(*) as never_used_memberships,\n",
    "    sum(total_spent) as total_amount_spent\n",
    "FROM `spark-kkbox.KKbox_User_Data.DRV_Jan2016`\n",
    "WHERE total_spent > 0 AND (total_secs = 0 or total_songs = 0 or total_logins = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8336 Paying Members who have an expiration date in Jan2016 who have never used the platform. These users have contributed $13,139,414 in pure revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 4.3***\n",
    "\n",
    "**spent_per_logins**: Total Spent / Total Logins\n",
    "\n",
    "**spent_per_secs**: Total Spent / Total Seconds\n",
    "\n",
    "**spent_per_num_unq**: Total Spent / Sum of all num_unq\n",
    "\n",
    "**spent_per_num_repeats**: Total Spent / Sum of all num_repats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Values to Zero to avoid NULLS\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET spent_per_logins = 0,\n",
    "                    spent_per_secs = 0,\n",
    "                    spent_per_song = 0,\n",
    "                    spent_per_num_unq = 0,\n",
    "                    spent_per_num_repeats = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate features with data\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET spent_per_logins = spent_per_logins1,\n",
    "                    spent_per_secs = spent_per_secs1,\n",
    "                    spent_per_song = spent_per_song1,\n",
    "                    spent_per_num_unq = spent_per_num_unq1,\n",
    "                    spent_per_num_repeats = spent_per_num_repeats1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                    msno as member,\n",
    "                    CASE WHEN never_active_subscriber = 0 AND total_spent_zero = 0 AND total_logins > 0 THEN total_spent / total_logins\n",
    "                    ELSE 0 END AS spent_per_logins1,\n",
    "                    CASE WHEN never_active_subscriber = 0 AND total_spent_zero = 0 AND total_secs > 0 THEN total_spent / total_secs\n",
    "                    ELSE 0 END AS spent_per_secs1,\n",
    "                    CASE WHEN never_active_subscriber = 0 AND total_spent_zero = 0 AND total_songs > 0 THEN total_spent / total_songs\n",
    "                    ELSE 0 END AS spent_per_song1,\n",
    "                    CASE WHEN never_active_subscriber = 0 AND total_spent_zero = 0 AND sum_num_unq > 0 THEN total_spent / sum_num_unq\n",
    "                    ELSE 0 END AS spent_per_num_unq1,\n",
    "                    CASE WHEN never_active_subscriber = 0 AND total_spent_zero = 0 AND sum_num_repeat > 0 THEN total_spent / sum_num_repeat\n",
    "                    ELSE 0 END AS spent_per_num_repeats1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.DRV_{month}`) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 4.4***\n",
    "\n",
    "- **total_cancelations**: Total Amount of Cancelations in Customer Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Values to Zero to avoid NULLS\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_cancelations = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate features with data\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET total_cancelations = total_cancelations1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                    t.msno as member,\n",
    "                    count (w.is_cancel) as total_cancelations1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.DRV_{month}` t\n",
    "                  JOIN `spark-kkbox.KKbox_User_Data.WRK_transactions_v1` w\n",
    "                  ON t.msno = w.msno\n",
    "                  WHERE w.transaction_date < t.membership_expire_date\n",
    "                  GROUP BY t.msno) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>5) Retrospective *Transaction* Table Features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intervals of the above - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>6) Aggregate *Member* Table Features</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 6.1***\n",
    "\n",
    "**city_agg**: Aggregation of cities based on population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Values to Zero to avoid NULLS\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET city_agg = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET city_agg = city_agg1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                    msno as member,\n",
    "                    CASE WHEN city = 1 THEN 1\n",
    "                         WHEN city = 13 THEN 2\n",
    "                         WHEN city = 5 THEN 3\n",
    "                         WHEN city in (4, 15, 22, 6) THEN 4\n",
    "                         ELSE 0 END AS city_agg1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.WRK_members_v3`) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 6.2***\n",
    "\n",
    "**payment_method_agg**: Aggregation of payment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all Values to Zero to avoid NULLS\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET payment_method_agg = 0\n",
    "                WHERE TRUE\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET payment_method_agg = payment_method_agg1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                    msno as member,\n",
    "                    CASE WHEN payment_method_id = 41 THEN 1\n",
    "                         WHEN payment_method_id in (40, 38, 39) THEN 2\n",
    "                         WHEN payment_method_id in (37, 34, 36) THEN 3\n",
    "                         WHEN payment_method_id in (33, 31) THEN 4\n",
    "                         ELSE 0 END AS payment_method_agg1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.DRV_{month}`) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>7) Calculating Churn Rate</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Batch 6.1***\n",
    "\n",
    "**is_churn**: Binary; Did the member not renew his subscription after this month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate features with data\n",
    "for month in months.keys():\n",
    "    QUERY = f\"\"\"\n",
    "                UPDATE `spark-kkbox.KKbox_User_Data.DRV_{month}` a \n",
    "                SET is_churn = is_churn1\n",
    "                FROM (\n",
    "                  SELECT \n",
    "                      t.msno as member,\n",
    "                      CASE WHEN max(w.membership_expire_date) = max(t.membership_expire_date) THEN 1\n",
    "                      ELSE 0 END AS is_churn1\n",
    "                  FROM `spark-kkbox.KKbox_User_Data.WRK_transactions_v1` t\n",
    "                  JOIN `spark-kkbox.KKbox_User_Data.DRV_{month}` w\n",
    "                  ON t.msno = w.msno\n",
    "                  GROUP BY t.msno) as sub_q \n",
    "                WHERE a.msno = sub_q.member\n",
    "             \"\"\"\n",
    "    \n",
    "    # Call .query() followed by .result() to trigger the 'lazy function'\n",
    "    bigquery_client.query(QUERY).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Check Datasets Created *(Iterative)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('KKBox-Churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If Working Locally on Computer, Importing Data Locally##\n",
    "\n",
    "# # Import DRV_Jan2016 (Train Set) from Google Cloud Storage via Pandas\n",
    "DRV_Jan2016_Balanced_1 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016_Balanced_1',inferSchema=True,header=True)\n",
    "DRV_Jan2016_Balanced_2 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016_Balanced_2',inferSchema=True,header=True)\n",
    "DRV_Jan2016_Balanced_3 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Jan2016_Balanced_3',inferSchema=True,header=True)\n",
    "\n",
    "# # # Import DRV_Feb2016 (Validation Set) from Google Cloud Storage via Pandas\n",
    "DRV_Feb2016 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Feb2016(1)',inferSchema=True,header=True)\n",
    "\n",
    "# Import DRV_Mar2016 (Test Set) from Google Cloud Storage via Pandas\n",
    "DRV_Mar2016 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Mar2016',inferSchema=True,header=True)\n",
    "\n",
    "DRV_Apr2016 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_Apr2016(1)',inferSchema=True,header=True)\n",
    "DRV_May2016 = spark.read.csv('D:\\J-5 Local\\Datasets_KKBox User Data_Monthly Datasets_DRV_May2016(1)',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Why is the Data missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of tables\n",
    "data_tables = {'DRV_Jan2016_Balanced_1': DRV_Jan2016_Balanced_1.columns,\n",
    "               'DRV_Jan2016_Balanced_2': DRV_Jan2016_Balanced_2.columns,\n",
    "               'DRV_Jan2016_Balanced_3': DRV_Jan2016_Balanced_3.columns,\n",
    "               'DRV_Feb2016': DRV_Feb2016.columns,\n",
    "               'DRV_Mar2016': DRV_Mar2016.columns,\n",
    "               'DRV_Apr2016': DRV_Apr2016.columns,\n",
    "               'DRV_May2016': DRV_May2016.columns,\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Detect Nonsense Values in Dataset: via Datatype</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_types = column_datatypes[column_datatypes['table_name'] == 'DRV_Jan20162']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with all table name and their respective features and feature datatypes\n",
    "column_datatypes = pd.DataFrame()\n",
    "\n",
    "QUERY = f\"\"\"\n",
    "            SELECT\n",
    "             *\n",
    "            FROM\n",
    "             `spark-kkbox.KKbox_User_Data`.INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE\n",
    "             table_name='DRV_Jan20162'\n",
    "         \"\"\"\n",
    "query = bigquery_client.query(QUERY)\n",
    "df = query.to_dataframe()\n",
    "column_datatypes = column_datatypes.append(df[['table_name','column_name','data_type']])\n",
    "    \n",
    "# Create a dictionary with all features and fill with their respective column and datatype pairs\n",
    "column_data_pairs = list(zip(column_datatypes['column_name'], column_datatypes['data_type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_data_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Each Column\n",
    "for x,y in data_tables.items():\n",
    "    print(f\"These are the problematic values for the Table: {x}\")\n",
    "    for column, dtype in column_data_pairs:\n",
    "        print(f'- {column} -')\n",
    "        QUERY = f\"\"\"\n",
    "                    SELECT\n",
    "                      msno,\n",
    "                      SAFE_CAST({column} as {dtype})\n",
    "                    FROM `spark-kkbox.KKbox_User_Data.{x}`\n",
    "                    WHERE SAFE_CAST({column} as {dtype}) IS NULL\n",
    "                 \"\"\"\n",
    "        print(bigquery_client.query(QUERY).to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Correctly Cast Columns</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_data_pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
