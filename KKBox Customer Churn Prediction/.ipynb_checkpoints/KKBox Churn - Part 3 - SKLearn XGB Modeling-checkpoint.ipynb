{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKBox Customer Churn Prediction\n",
    "### w/ XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: <font color=green>*Model Creation and Evaluation*</font>\n",
    "Please refer to the following article for a comprehensive review of the project: XXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "from __future__ import absolute_import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Working Locally\n",
    "\n",
    "# Import Subsamples\n",
    "DRV_Jan2016_1to1 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_1to1_clust')\n",
    "DRV_Jan2016_3to1 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_3to1_clust')\n",
    "DRV_Jan2016_5to1 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_5to1_clust')\n",
    "DRV_Jan2016_7to1 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_7to1_clust')\n",
    "DRV_Jan2016_9to1 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_9to1_clust')\n",
    "DRV_Jan2016_11to1 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_11to1_clust')\n",
    "DRV_Jan2016_13to1 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_13to1_clust')\n",
    "\n",
    "# Import Main Sets\n",
    "DRV_Jan2016 = pd.read_csv('D:\\J-5 Local\\DRV_Jan2016_With_Cluster')\n",
    "\n",
    "# Import DRV_Feb2016 (Validation Set) \n",
    "DRV_Feb2016 = pd.read_csv('D:\\J-5 Local\\DRV_Feb2016_With_Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Data Pre-Processing</font> -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>*Split Feautres by Categorical or Continuous*</font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of Categorical feature names\n",
    "cat_feats = ['is_auto_renew', 'total_spent_zero', 'city_agg', 'payment_method_agg', 'never_active_subscriber', 'Cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>*Initial Feature Selection*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all un-needed columns\n",
    "DRV_Jan2016 = DRV_Jan2016.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "DRV_Feb2016 = DRV_Feb2016.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "\n",
    "DRV_Jan2016_1to1 = DRV_Jan2016_1to1.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "DRV_Jan2016_3to1 = DRV_Jan2016_3to1.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "DRV_Jan2016_5to1 = DRV_Jan2016_5to1.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "DRV_Jan2016_7to1 = DRV_Jan2016_7to1.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "DRV_Jan2016_9to1 = DRV_Jan2016_9to1.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "DRV_Jan2016_11to1 = DRV_Jan2016_11to1.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)\n",
    "DRV_Jan2016_13to1 = DRV_Jan2016_13to1.drop(['Unnamed: 0', 'msno', 'membership_expire_date', 'is_net_paid_amount', 'registration_init_time','registration_init_time',\n",
    "                          'city','bd','payment_method_id','registered_via'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>*Encode Categorical Variables*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded all categoricals\n",
    "DRV_Jan2016 = pd.get_dummies(DRV_Jan2016, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "DRV_Feb2016 = pd.get_dummies(DRV_Feb2016, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "\n",
    "DRV_Jan2016_1to1 = pd.get_dummies(DRV_Jan2016_1to1, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "DRV_Jan2016_3to1 = pd.get_dummies(DRV_Jan2016_3to1, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "DRV_Jan2016_5to1 = pd.get_dummies(DRV_Jan2016_5to1, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "DRV_Jan2016_7to1 = pd.get_dummies(DRV_Jan2016_7to1, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "DRV_Jan2016_9to1 = pd.get_dummies(DRV_Jan2016_9to1, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "DRV_Jan2016_11to1 = pd.get_dummies(DRV_Jan2016_11to1, prefix=cat_feats, columns=cat_feats, drop_first=True)\n",
    "DRV_Jan2016_13to1 = pd.get_dummies(DRV_Jan2016_13to1, prefix=cat_feats, columns=cat_feats, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>*Feature Scaling*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Scaler Object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# # Scale Train and Validation Sets\n",
    "DRV_Jan2016_scaled = scaler.fit_transform(DRV_Jan2016.drop('is_churn', axis=1))\n",
    "DRV_Feb2016_scaled = scaler.fit_transform(DRV_Feb2016.drop('is_churn', axis=1))\n",
    "\n",
    "# Scale Split Sets\n",
    "DRV_Jan2016_1to1_scaled = scaler.fit_transform(DRV_Jan2016_1to1.drop('is_churn', axis=1))\n",
    "DRV_Jan2016_3to1_scaled = scaler.fit_transform(DRV_Jan2016_3to1.drop('is_churn', axis=1))\n",
    "DRV_Jan2016_5to1_scaled = scaler.fit_transform(DRV_Jan2016_5to1.drop('is_churn', axis=1))\n",
    "DRV_Jan2016_7to1_scaled = scaler.fit_transform(DRV_Jan2016_7to1.drop('is_churn', axis=1))\n",
    "DRV_Jan2016_9to1_scaled = scaler.fit_transform(DRV_Jan2016_9to1.drop('is_churn', axis=1))\n",
    "DRV_Jan2016_11to1_scaled = scaler.fit_transform(DRV_Jan2016_11to1.drop('is_churn', axis=1))\n",
    "DRV_Jan2016_13to1_scaled = scaler.fit_transform(DRV_Jan2016_13to1.drop('is_churn', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38013, 239)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DRV_Jan2016_1to1_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264393, 239)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DRV_Jan2016_13to1_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation: Pipeline and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - <font color=blue>Model Tuning</font> -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>XGBOOST Parameter Tuning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Param Grid\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [.1, .075, .05, .025, .01],\n",
    "        'n_estimators': [100, 250, 500, 750, 1000]\n",
    "        }\n",
    "\n",
    "# Instatiate Esitmator Object\n",
    "xgb = XGBClassifier(objective='binary:logistic', n_jobs=1)\n",
    "\n",
    "# Instatiate StratKFold Object\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=4, shuffle = True)\n",
    "\n",
    "# # Instatiate Random Search CV Object\n",
    "# rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "#                                    n_jobs=4, cv=skf, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Create Custom Evaluator***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator that we can use instead of BinaryClassificationEvaluator() in grid search\n",
    "class ClassEvaluatorPandas:\n",
    "\n",
    "    def __init__(self, modelname, model, y_pred, y_true):\n",
    "        \n",
    "        # Initialize variables\n",
    "        self.modelname = modelname\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        self.model = model\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        self.cm = confusion_matrix(y_true,y_pred)\n",
    "        \n",
    "        # Calculate confusion matrix values\n",
    "        self.tp = self.cm[0][0]\n",
    "        self.fp = self.cm[0][1]\n",
    "        self.tn = self.cm[1][1]\n",
    "        self.fn = self.cm[1][0]\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \n",
    "        # Calculate Metrics and add epsilon to prevent division by zero\n",
    "        precision = self.tp / float(self.tp + self.fp + 0.00001)\n",
    "        recall = self.tp / float(self.tp + self.fn + 0.00001)\n",
    "        f1 = (2 * precision * recall) / float(precision + recall + 0.00001)\n",
    "        error = (self.fp + self.fn + 0.00001) / (self.tp + self.fp + self.tn + self.fn + 0.00001)\n",
    "        \n",
    "        # Instantiate Evaluator and call AUC metric\n",
    "        from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "        \n",
    "        AUC = roc_auc_score(self.y_true, self.y_pred)\n",
    "        \n",
    "        return pd.DataFrame(data=[[self.modelname, AUC, f1, precision, recall, error]], \n",
    "                            columns=['modelname', 'AUC', 'f1', 'precision', 'recall', 'error'])\n",
    "    \n",
    "    def confusionmatrix(self):\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        # Print Confusion Matrix\n",
    "        return self.cm\n",
    "        \n",
    "    \n",
    "    def modelparams(self):\n",
    "        scores = self.model.avgMetrics\n",
    "        params = [{p.name: v for p, v in m.items()} for m in self.model.getEstimatorParamMaps()]\n",
    "        params_pd = pd.DataFrame(params)\n",
    "        params_pd['AUC score'] = scores\n",
    "        return params_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Execution and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Train Model: All Splits, XGB + GridCV </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8, score=0.902, total= 3.3min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8, score=0.910, total= 3.5min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8, score=0.910, total= 3.6min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8 \n",
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=0.5, colsample_bytree=0.8, score=0.907, total= 3.4min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0, score=0.903, total= 2.7min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0, score=0.909, total= 2.5min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0, score=0.910, total= 2.5min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0 \n",
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=0.5, colsample_bytree=1.0, score=0.906, total= 2.3min\n",
      "[CV] subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6 \n",
      "[CV]  subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6, score=0.898, total=  50.8s\n",
      "[CV] subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6 \n",
      "[CV]  subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6, score=0.904, total=  51.0s\n",
      "[CV] subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6 \n",
      "[CV]  subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6, score=0.909, total=  50.8s\n",
      "[CV] subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6 \n",
      "[CV]  subsample=1.0, n_estimators=250, min_child_weight=1, max_depth=3, learning_rate=0.05, gamma=5, colsample_bytree=0.6, score=0.903, total=  51.0s\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=1, colsample_bytree=0.6 \n",
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=1, colsample_bytree=0.6, score=0.903, total= 2.5min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=1, colsample_bytree=0.6 \n",
      "[CV]  subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=1, colsample_bytree=0.6, score=0.909, total= 2.5min\n",
      "[CV] subsample=1.0, n_estimators=500, min_child_weight=5, max_depth=5, learning_rate=0.075, gamma=1, colsample_bytree=0.6 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "                                   n_jobs=1, cv=skf, verbose=3)\n",
    "gbx1to1 = rscv.fit(DRV_Jan2016_1to1_scaled, DRV_Jan2016_1to1['is_churn'])\n",
    "end = time.time()\n",
    "print('Time spent for training: {}'.format(round(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "                                   n_jobs=1, cv=skf, verbose=3)\n",
    "gbx3to1 = rscv.fit(DRV_Jan2016_3to1_scaled, DRV_Jan2016_3to1['is_churn'])\n",
    "end = time.time()\n",
    "print('Time spent for training: {}'.format(round(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "                                   n_jobs=1, cv=skf, verbose=3)\n",
    "gbx5to1 = rscv.fit(DRV_Jan2016_5to1_scaled, DRV_Jan2016_5to1['is_churn'])\n",
    "end = time.time()\n",
    "print('Time spent for training: {}'.format(round(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "                                   n_jobs=4, cv=skf, verbose=3)\n",
    "gbx7to1 = rscv.fit(DRV_Jan2016_7to1_scaled, DRV_Jan2016_7to1['is_churn'])\n",
    "end = time.time()\n",
    "print('Time spent for training: {}'.format(round(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "                                   n_jobs=4, cv=skf, verbose=3)\n",
    "gbx9to1 = rscv.fit(DRV_Jan2016_9to1_scaled, DRV_Jan2016_9to1['is_churn'])\n",
    "end = time.time()\n",
    "print('Time spent for training: {}'.format(round(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "                                   n_jobs=4, cv=skf, verbose=3)\n",
    "gbx11to1 = rscv.fit(DRV_Jan2016_11to1_scaled, DRV_Jan2016_11to1['is_churn'])\n",
    "end = time.time()\n",
    "print('Time spent for training: {}'.format(round(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rscv = RandomizedSearchCV(xgb, param_distributions=params, n_iter=5, scoring='recall', \n",
    "                                   n_jobs=4, cv=skf, verbose=3)\n",
    "gbx13to1 = rscv.fit(DRV_Jan2016_13to1_scaled, DRV_Jan2016_13to1['is_churn'])\n",
    "end = time.time()\n",
    "print('Time spent for training: {}'.format(round(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of Models that were created\n",
    "ensembles_created = {\n",
    "                  'gbx1to1' : gbx1to1,\n",
    "                  'gbx3to1' : gbx3to1,\n",
    "                  'gbx5to1' : gbx5to1,\n",
    "                  'gbx7to1' : gbx7to1,\n",
    "                  'gbx9to1' : gbx9to1,\n",
    "                  'gbx11to1' : gbx11to1,\n",
    "                  'gbx13to1' : gbx13to1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Evaluate Train Model: All Splits, XGB + GridCV </font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Results\n",
    "train_all_results = pd.DataFrame()\n",
    "\n",
    "for model_name, model1 in ensembles_created.items():\n",
    "    \n",
    "    # Temporary Variables for our Loop\n",
    "    temp = model1.predict(DRV_Jan2016_scaled)\n",
    "    temp_class = ClassEvaluatorPandas(modelname=model_name, model=model1, y_pred=temp, y_true=DRV_Jan2016['is_churn'])\n",
    "   \n",
    "    # Create a Dataframe of Train Results and Print Confusion Matrixes\n",
    "    train_all_results = train_all_results.append(temp_class.evaluate())\n",
    "    print('{}'.format(model_name))\n",
    "    print(temp_class.confusionmatrix())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - <font color=blue>Evaluate Valuation Model: All Splits, XGB + GridCV </font> -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Model Results\n",
    "validation_all_results = pd.DataFrame()\n",
    "\n",
    "# Create a Dataframe of Validation Results and Print Confusion Matrixes\n",
    "for model_name, model1 in ensembles_created.items():\n",
    "    \n",
    "    # Temporary Variables for our Loop\n",
    "    temp = model1.predict(DRV_Feb2016_scaled)\n",
    "    temp_class = ClassEvaluatorPandas(modelname=model_name, model=model1, y_pred=temp, y_true=DRV_Feb2016['is_churn'])\n",
    "\n",
    "    # Validation Results and Print Confusion Matrixes\n",
    "    validation_all_results = validation_all_results.append(temp_class.evaluate())\n",
    "    print('{}'.format(model_name))\n",
    "    print(temp_class.confusionmatrix())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=purple>Generalization Between Train and Validation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = train_all_results[train_all_results.columns[1:]] - validation_all_results[validation_all_results.columns[1:]]\n",
    "results_all['modelname'] = train_all_results['modelname']\n",
    "results_all.sort_values('AUC', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
